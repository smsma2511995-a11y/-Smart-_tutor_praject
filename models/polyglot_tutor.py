# models/polyglot_tutor.py
import torch
import torch.nn as nn
from transformers import PreTrainedModel, PretrainedConfig
import json
import re
from typing import Dict, List, Optional
import numpy as np

from .polyglot_model import PolyglotTutorConfig, PolyglotTutorModel
from .knowledge_base import EducationalKnowledgeBase
from .answer_generator import SmartAnswerGenerator

class PolyglotEducationalAI:
    def __init__(self, model_path=None):
        # Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        self.config = PolyglotTutorConfig()
        self.model = PolyglotTutorModel(self.config)
        self.knowledge_base = EducationalKnowledgeBase()
        self.answer_generator = SmartAnswerGenerator(self.knowledge_base)
        self.tokenizer = self.create_optimized_tokenizer()
        
        # ÙˆØ¶Ø¹ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
        self.model.eval()
        
        if model_path:
            self.load_model(model_path)
    
    def create_optimized_tokenizer(self):
        """Ø¥Ù†Ø´Ø§Ø¡ tokenizer Ù…Ø¨Ø³Ø· ÙˆÙØ¹Ø§Ù„"""
        vocab = {
            # Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø®Ø§ØµØ©
            "[PAD]": 0, "[UNK]": 1, "[CLS]": 2, "[SEP]": 3, "[MASK]": 4,
        }
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        educational_terms = [
            # Ø¹Ø±Ø¨ÙŠØ©
            "Ø´Ø±Ø­", "Ù…Ø«Ø§Ù„", "Ù‚Ø§Ø¹Ø¯Ø©", "Ù†Ø¸Ø±ÙŠØ©", "Ù‚Ø§Ù†ÙˆÙ†", "Ù…Ø¹Ø§Ø¯Ù„Ø©", "Ù…Ø³Ø£Ù„Ø©", 
            "Ø­Ù„", "Ø¥Ø¬Ø§Ø¨Ø©", "Ø³Ø¤Ø§Ù„", "Ø¬ÙˆØ§Ø¨", "ØªØ­Ù„ÙŠÙ„", "Ø§Ø³ØªÙ†ØªØ§Ø¬", "ØªØ¹Ø±ÙŠÙ",
            "Ù…ÙÙ‡ÙˆÙ…", "Ø£Ø³Ø§Ø³ÙŠ", "Ù…ØªÙ‚Ø¯Ù…", "ØªÙ…Ø±ÙŠÙ†", "Ø§Ù…ØªØ­Ø§Ù†", "Ø¯Ø±Ø³", "ØªØ¹Ù„Ù…",
            "Ø·Ø§Ù„Ø¨", "Ù…Ø¹Ù„Ù…", "Ù…Ø¯Ø±Ø³Ø©", "Ø¬Ø§Ù…Ø¹Ø©", "ØªØ¹Ù„ÙŠÙ…", "Ø¯Ø±Ø§Ø³Ø©", "Ø¨Ø­Ø«",
            
            # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            "explain", "example", "rule", "theory", "law", "equation", "problem",
            "solution", "answer", "question", "analysis", "conclusion", "definition",
            "concept", "basic", "advanced", "exercise", "exam", "lesson", "learn",
            "student", "teacher", "school", "university", "education", "study", "research",
            
            # ÙØ±Ù†Ø³ÙŠØ©
            "expliquez", "exemple", "rÃ¨gle", "thÃ©orie", "loi", "Ã©quation", "problÃ¨me",
            "solution", "rÃ©ponse", "question", "analyse", "conclusion", "dÃ©finition",
            "concept", "basique", "avancÃ©", "exercice", "examen", "leÃ§on", "apprendre",
            "Ã©tudiant", "professeur", "Ã©cole", "universitÃ©", "Ã©ducation", "Ã©tude", "recherche"
        ]
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        base_words = [
            # Ø¹Ø±Ø¨ÙŠØ©
            "Ø§Ù„", "ÙÙŠ", "Ù…Ù†", "Ø¹Ù„Ù‰", "Ø¥Ù„Ù‰", "Ù…Ø¹", "Ù…Ø§", "Ù‡Ùˆ", "Ù‡ÙŠ", "ÙƒØ§Ù†", 
            "ÙŠÙƒÙˆÙ†", "ÙƒØ§Ù†Øª", "Ø£Ù†Ø§", "Ø£Ù†Øª", "Ù‡Ùˆ", "Ù‡ÙŠ", "Ù†Ø­Ù†", "Ù‡Ù…", "Ù‡Ø°Ø§", "Ù‡Ø°Ù‡",
            "Ø°Ù„Ùƒ", "ØªÙ„Ùƒ", "Ø£ÙŠÙ†", "Ù…ØªÙ‰", "ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "ÙƒÙ…", "Ø£ÙŠ", "ÙƒÙ„", "Ø¨Ø¹Ø¶",
            
            # Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            "the", "is", "are", "what", "how", "why", "when", "where", "who",
            "I", "you", "he", "she", "we", "they", "it", "this", "that", "these",
            "those", "which", "all", "some", "any", "many", "few", "much", "little",
            
            # ÙØ±Ù†Ø³ÙŠØ©
            "le", "la", "est", "que", "comment", "pourquoi", "quand", "oÃ¹", "qui",
            "je", "tu", "il", "elle", "nous", "vous", "ils", "ce", "cette", "ces",
            "tout", "quelque", "aucun", "plusieurs", "peu", "beaucoup", "trÃ¨s"
        ]
        
        # Ø¯Ù…Ø¬ ÙƒÙ„ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
        all_terms = educational_terms + base_words
        current_index = len(vocab)
        
        for term in all_terms:
            if term not in vocab:
                vocab[term] = current_index
                current_index += 1
        
        # Ø¥Ø¶Ø§ÙØ© Ø£Ø±Ù‚Ø§Ù…
        for i in range(10):
            vocab[str(i)] = current_index + i
        
        return vocab
    
    def preprocess_text(self, text):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ ÙØ¹Ø§Ù„"""
        if not text or not text.strip():
            return [self.tokenizer["[CLS]"], self.tokenizer["[SEP]"]]
        
        words = text.split()
        tokens = [self.tokenizer["[CLS]"]]
        
        for word in words:
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø© ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
            clean_word = ''.join(c for c in word if c.isalnum() or c in [' ', '-', '_', '.', '?', '!']).strip()
            if clean_word:
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø© Ù„Ù„ØªÙ‚Ù„ÙŠÙ„ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
                clean_word_lower = clean_word.lower()
                token = self.tokenizer.get(clean_word_lower, self.tokenizer["[UNK]"])
                tokens.append(token)
        
        # ØªÙ‚Ù„ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø³Ù…ÙˆØ­
        max_length = self.config.max_position_embeddings - 1  # Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù€ [SEP]
        if len(tokens) > max_length:
            tokens = tokens[:max_length]
        
        tokens.append(self.tokenizer["[SEP]"])
        return tokens
    
    def ask_question(self, question, target_language="ar"):
        """Ø§Ù„ÙˆØ¸ÙŠÙØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ø·Ø±Ø­ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"""
        if not question or not question.strip():
            return {
                "answer": "ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„ ÙˆØ§Ø¶Ø­.",
                "detected_language": "unknown",
                "detected_subject": "general",
                "target_language": target_language,
                "confidence": 0.0
            }
        
        try:
            # ÙƒØ´Ù Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ù…Ø§Ø¯Ø©
            source_language = self.knowledge_base.detect_language(question)
            subject = self.knowledge_base.detect_subject(question, source_language)
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„
            tokens = self.preprocess_text(question)
            input_ids = torch.tensor([tokens])
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ù†Ø§Ø¹ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
            attention_mask = (input_ids != self.tokenizer["[PAD]"]).float()
            
            # ØªØ­Ø¯ÙŠØ¯ Ù„ØºØ© Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„
            lang_id = torch.tensor([self.knowledge_base.language_codes.get(source_language, 0)])
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            with torch.no_grad():
                outputs = self.model(input_ids, attention_mask=attention_mask, language_id=lang_id)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒÙŠ
            answer = self.answer_generator.generate_response(
                question, subject, source_language, target_language
            )
            
            return {
                "answer": answer,
                "detected_language": source_language,
                "detected_subject": subject,
                "target_language": target_language,
                "confidence": self._calculate_confidence(outputs, question)
            }
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¤Ø§Ù„: {e}")
            return {
                "answer": "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø³Ø¤Ø§Ù„Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.",
                "detected_language": "unknown",
                "detected_subject": "general",
                "target_language": target_language,
                "confidence": 0.0
            }
    
    def _calculate_confidence(self, outputs, question):
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©"""
        try:
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ù…Ù† ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ù…Ø§Ø¯Ø©
            lang_probs = torch.softmax(outputs['language_logits'], dim=-1)
            subject_probs = torch.softmax(outputs['subject_logits'], dim=-1)
            
            lang_confidence = torch.max(lang_probs).item()
            subject_confidence = torch.max(subject_probs).item()
            
            # Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¬Ø­ Ù„Ù„Ø«Ù‚Ø©
            confidence = (lang_confidence * 0.4) + (subject_confidence * 0.6)
            
            # Ø¶Ø¨Ø· Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø·ÙˆÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØªØ¹Ù‚ÙŠØ¯Ù‡
            question_complexity = min(len(question.split()) / 20, 1.0)  # ØªØ¹Ù‚ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
            adjusted_confidence = confidence * (0.7 + 0.3 * question_complexity)
            
            return min(0.95, adjusted_confidence)
            
        except Exception:
            return 0.7  # Ø«Ù‚Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
    
    def save_model(self, path):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„ØªÙƒÙ…ÙŠÙ… Ù„ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø­Ø¬Ù…"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… float16 Ù„ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
            state_dict_fp16 = {k: v.half() for k, v in self.model.state_dict().items()}
            
            torch.save({
                'model_state_dict': state_dict_fp16,
                'config': self.config,
                'tokenizer': self.tokenizer
            }, path)
            
            # Ø­Ø³Ø§Ø¨ ÙˆØ­Ø¬Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            model_size = sum(p.numel() for p in self.model.parameters())
            file_size_mb = model_size * 2 / 1024 / 1024  # float16 = 2 bytes per parameter
            
            print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
            print(f"ğŸ“Š Ø­Ø¬Ù… Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª: {model_size:,} Ù…Ø¹Ù„Ù…Ø©")
            print(f"ğŸ’¾ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ Ù„Ù„Ù…Ù„Ù: {file_size_mb:.1f} Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª")
            
            return True
            
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")
            return False
    
    def load_model(self, path):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        try:
            checkpoint = torch.load(path, map_location='cpu')
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print("âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ù†Ø¬Ø§Ø­!")
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: {e}")

# Ø¯Ø§Ù„Ø© Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø³Ø±ÙŠØ¹
def test_polyglot_ai():
    """Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
    print("ğŸ§ª Ø¨Ø¯Ø¡ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ...")
    
    ai = PolyglotEducationalAI()
    
    test_questions = [
        "What is algebra?",
        "Ù…Ø§ Ù‡ÙŠ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©ØŸ",
        "Expliquez la grammaire franÃ§aise",
        "ÙƒÙŠÙ Ø£Ø­Ù„ Ù…Ø¹Ø§Ø¯Ù„Ø© Ø±ÙŠØ§Ø¶ÙŠØ©ØŸ",
        "Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø¨Ø± ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø©ØŸ"
    ]
    
    for question in test_questions:
        print(f"\n{'='*60}")
        print(f"â“ Ø§Ù„Ø³Ø¤Ø§Ù„: {question}")
        
        result = ai.ask_question(question, "ar")
        
        print(f"ğŸŒ Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {result['detected_language']}")
        print(f"ğŸ“š Ø§Ù„Ù…Ø§Ø¯Ø© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {result['detected_subject']}")
        print(f"ğŸ¯ Ù„ØºØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {result['target_language']}")
        print(f"ğŸ“Š Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {result['confidence']:.2f}")
        print(f"ğŸ’¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n{result['answer']}")
    
    # Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    ai.save_model("mini_polyglot_tutor.pth")

if __name__ == "__main__":
    test_polyglot_ai()
