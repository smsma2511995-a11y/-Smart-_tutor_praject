# core/document_processor.py
import os
import sqlite3
import json
import re
import time
from pathlib import Path
from threading import Thread
from urllib.parse import urljoin, urlparse
from typing import Dict, List, Optional, Tuple

try:
    import fitz  # PyMuPDF
except ImportError:
    print("âš ï¸ ØªÙ†Ø¨ÙŠÙ‡: PyMuPDF ØºÙŠØ± Ù…Ø«Ø¨Øª. Ø³ÙŠØªÙ… ØªØ¹Ø·ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬Ø© PDF.")
    fitz = None

import requests
from bs4 import BeautifulSoup
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class DocumentProcessor:
    def __init__(self, base_dir: str = "smarttutor_data"):
        self.base_dir = Path(base_dir)
        self.setup_directories()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
        self.model_name = "all-MiniLM-L6-v2"
        self.model = SentenceTransformer(self.model_name)
        
        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        self.chunk_size = 400
        self.chunk_overlap = 50
        self.top_k = 5
        self.online_enabled = True
        
        # Ø§Ù„ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.ram_embs = {}
        self.ram_chunks = {}
        self.ram_knowledge = {}
        
        print("âœ… ØªÙ… ØªÙ‡ÙŠØ¦Ø© Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù†Ø¬Ø§Ø­")
    
    def setup_directories(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù„Ø§Ø²Ù…Ø©"""
        dirs = [
            self.base_dir / "books",
            self.base_dir / "embeddings", 
            self.base_dir / "knowledge",
            self.base_dir / "database"
        ]
        
        for dir_path in dirs:
            dir_path.mkdir(parents=True, exist_ok=True)
        
        self.db_path = self.base_dir / "database" / "metadata.db"
        self.init_database()
    
    def init_database(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        
        c.execute('''
            CREATE TABLE IF NOT EXISTS books (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT, path TEXT, subject TEXT,
                num_chunks INTEGER, emb_file TEXT, chunks_file TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        c.execute('''
            CREATE TABLE IF NOT EXISTS qna (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT, answer TEXT, subject TEXT,
                confidence REAL, meta TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
    
    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ù…Ù„Ù PDF"""
        if fitz is None:
            return ""
        
        try:
            doc = fitz.open(pdf_path)
            text_parts = []
            
            for page in doc:
                try:
                    text = page.get_text()
                    if text.strip():
                        text_parts.append(text)
                except Exception:
                    continue
            
            doc.close()
            return "\n".join(text_parts)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† PDF: {e}")
            return ""
    
    def read_text_file(self, file_path: str) -> str:
        """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ù†ØµÙŠ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            try:
                with open(file_path, 'r', encoding='latin-1') as f:
                    return f.read()
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù†ØµÙŠ: {e}")
                return ""
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")
            return ""
    
    def chunk_text(self, text: str, size: int = None, overlap: int = None) -> List[str]:
        """ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡"""
        if not text:
            return []
        
        size = size or self.chunk_size
        overlap = overlap or self.chunk_overlap
        
        text = text.replace('\r', '').strip()
        chunks = []
        i = 0
        text_length = len(text)
        
        while i < text_length:
            end = min(i + size, text_length)
            chunk = text[i:end].strip()
            
            if len(chunk) > 30:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹
                chunks.append(chunk)
            
            i = max(end - overlap, end)
            if i == end:  # ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„Ù„Ø§Ù†Ù‡Ø§Ø¦ÙŠ
                break
        
        return chunks
    
    def safe_filename(self, text: str) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¢Ù…Ù†"""
        safe = re.sub(r'[^\w\s\-_.]', '_', text)
        return safe[:120]
    
    def embed_texts(self, texts: List[str]) -> np.ndarray:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØ¶Ù…ÙŠÙ†Ø§Øª Ù„Ù„Ù†ØµÙˆØµ"""
        if not texts:
            return np.array([])
        
        embeddings = self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True)
        return embeddings.astype(np.float32)
    
    def add_document(self, file_path: str, subject: str = "general") -> Tuple[bool, str]:
        """Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªÙ†Ø¯ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ù†Ø¸Ø§Ù…"""
        file_path = Path(file_path)
        if not file_path.exists():
            return False, "Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯"
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        if file_path.suffix.lower() == '.pdf':
            text = self.extract_text_from_pdf(str(file_path))
        else:
            text = self.read_text_file(str(file_path))
        
        if not text.strip():
            return False, "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Øµ Ù…Ù† Ø§Ù„Ù…Ù„Ù"
        
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡
        chunks = self.chunk_text(text)
        if not chunks:
            return False, "Ù„Ø§ ÙŠÙ…ÙƒÙ† ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù…Ù†Ø§Ø³Ø¨Ø©"
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ¶Ù…ÙŠÙ†Ø§Øª
        embeddings = self.embed_texts(chunks)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        title = self.safe_filename(file_path.stem)
        
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        self.ram_embs[title] = embeddings
        self.ram_chunks[title] = chunks
        
        # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        conn = sqlite3.connect(self.db_path)
        c = conn.cursor()
        
        c.execute('''
            INSERT INTO books (title, path, subject, num_chunks)
            VALUES (?, ?, ?, ?)
        ''', (title, str(file_path), subject, len(chunks)))
        
        conn.commit()
        conn.close()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        self.extract_and_save_concepts(chunks, subject, title)
        
        return True, f"ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³ØªÙ†Ø¯: {title} ({len(chunks)} Ø¬Ø²Ø¡)"
    
    def extract_and_save_concepts(self, chunks: List[str], subject: str, source: str):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ­ÙØ¸ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ù† Ø§Ù„Ù†Øµ"""
        # Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© Ù…Ù† Ø§Ù„Ù‚Ø·Ø¹ Ù„Ù„ØªØ­Ù„ÙŠÙ„
        sample_chunks = chunks[:min(20, len(chunks))]
        sample_text = "\n".join(sample_chunks)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¬Ù…Ù„ Ø§Ù„Ù…Ø±Ø´Ø­Ø© ÙƒÙ„Ù…ÙØ§Øª
        sentences = re.split(r'[.\n!?Ø›ØŸ]+', sample_text)
        candidates = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
            
            words = sentence.split()
            if 3 <= len(words) <= 25:  # Ø¬Ù…Ù„ Ù…Ø¹Ù‚ÙˆÙ„Ø© Ø§Ù„Ø·ÙˆÙ„
                candidates.append(sentence[:200])  # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø·ÙˆÙ„
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        unique_candidates = list(set(candidates))
        
        # Ø­ÙØ¸ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        if subject not in self.ram_knowledge:
            self.ram_knowledge[subject] = {}
        
        for concept in unique_candidates:
            if concept not in self.ram_knowledge[subject]:
                self.ram_knowledge[subject][concept] = {
                    "explain": concept,
                    "weight": 1,
                    "sources": [source]
                }
            else:
                self.ram_knowledge[subject][concept]["weight"] += 1
                if source not in self.ram_knowledge[subject][concept]["sources"]:
                    self.ram_knowledge[subject][concept]["sources"].append(source)
    
    def search_documents(self, question: str, subject: str = None, top_k: int = None) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¹Ù† Ø¥Ø¬Ø§Ø¨Ø© Ù„Ù„Ø³Ø¤Ø§Ù„"""
        top_k = top_k or self.top_k
        
        # ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø³Ø¤Ø§Ù„
        question_embedding = self.model.encode([question], convert_to_numpy=True)
        
        results = []
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù‚Ø·Ø¹ Ø§Ù„Ù…Ø®Ø²Ù†Ø©
        for title, embeddings in self.ram_embs.items():
            if embeddings.shape[0] == 0:
                continue
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
            similarities = cosine_similarity(question_embedding, embeddings)[0]
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            for idx in top_indices:
                if idx < len(self.ram_chunks[title]):
                    results.append({
                        'type': 'document',
                        'title': title,
                        'subject': subject or 'general',
                        'score': float(similarities[idx]),
                        'content': self.ram_chunks[title][idx],
                        'source': 'document_corpus'
                    })
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
        if subject and subject in self.ram_knowledge:
            knowledge_items = list(self.ram_knowledge[subject].items())
            if knowledge_items:
                concepts = [item[0] for item in knowledge_items]
                concept_embeddings = self.model.encode(concepts, convert_to_numpy=True)
                
                concept_similarities = cosine_similarity(question_embedding, concept_embeddings)[0]
                top_concept_indices = np.argsort(concept_similarities)[::-1][:top_k]
                
                for idx in top_concept_indices:
                    concept, data = knowledge_items[idx]
                    results.append({
                        'type': 'concept',
                        'title': concept,
                        'subject': subject,
                        'score': float(concept_similarities[idx]),
                        'content': data['explain'],
                        'source': 'knowledge_base'
                    })
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø­Ø³Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        results.sort(key=lambda x: x['score'], reverse=True)
        return results[:top_k]
    
    def fetch_web_content(self, url: str) -> Tuple[Optional[str], Optional[str]]:
        """Ø¬Ù„Ø¨ Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„ÙˆÙŠØ¨"""
        if not self.online_enabled:
            return None, "Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø£ÙˆÙ†Ù„Ø§ÙŠÙ† Ù…Ø¹Ø·Ù„"
        
        try:
            response = requests.get(url, timeout=10, headers={"User-Agent": "SmartTutor/1.0"})
            if response.status_code != 200:
                return None, f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {response.status_code}"
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
            title_tag = soup.find('title')
            title = title_tag.get_text().strip() if title_tag else "Ù…Ø­ØªÙˆÙ‰ ÙˆÙŠØ¨"
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
            article = soup.find('article')
            if article:
                content = article.get_text(separator='\n')
            else:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù† Ø§Ù„ÙÙ‚Ø±Ø§Øª
                paragraphs = soup.find_all('p')
                content = '\n'.join([p.get_text() for p in paragraphs if p.get_text().strip()])
            
            if not content.strip():
                return None, "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…Ø­ØªÙˆÙ‰ Ù†ØµÙŠ ÙÙŠ Ø§Ù„ØµÙØ­Ø©"
            
            return content, title
            
        except Exception as e:
            return None, f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {e}"

# Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ø¸Ø§Ù…
def test_document_processor():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª"""
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª...")
    
    processor = DocumentProcessor()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø¥Ø¶Ø§ÙØ© Ù…Ø³ØªÙ†Ø¯ Ù†ØµÙŠ Ø¨Ø³ÙŠØ·
    test_content = """
    Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø§Ù„Ø°ÙŠ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª.
    ÙÙŠ Ø§Ù„Ø¬Ø¨Ø±ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø±ÙˆÙ Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„Ø©.
    Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„Ø¬Ø¨Ø± Ù‡ÙŠ: Ø³ + Ù¢ = Ù¥ØŒ Ø­ÙŠØ« Ø³ = Ù£.
    
    Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© ØªØ¯Ø±Ø³ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„ÙØ¶Ø§Ø¡ ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ù…Ø§.
    Ù…Ù† Ø§Ù„Ø£Ø´ÙƒØ§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©: Ø§Ù„Ù…Ø±Ø¨Ø¹ØŒ ÙˆØ§Ù„Ù…Ø³ØªØ·ÙŠÙ„ØŒ ÙˆØ§Ù„Ù…Ø«Ù„Ø«.
    """
    
    # Ø­ÙØ¸ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± ÙÙŠ Ù…Ù„Ù
    test_file = "test_document.txt"
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write(test_content)
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³ØªÙ†Ø¯
    success, message = processor.add_document(test_file, "math")
    print(f"Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¥Ø¶Ø§ÙØ©: {message}")
    
    if success:
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¨Ø­Ø«
        results = processor.search_documents("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¬Ø¨Ø±ØŸ", "math")
        print(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬: {len(results)}")
        
        for i, result in enumerate(results, 1):
            print(f"Ù†ØªÙŠØ¬Ø© {i}: {result['content'][:100]}...")
    
    # ØªÙ†Ø¸ÙŠÙ
    if os.path.exists(test_file):
        os.remove(test_file)

if __name__ == "__main__":
    test_document_processor()
