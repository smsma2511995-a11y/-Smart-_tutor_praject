# ğŸš€ Ø¥Ù†Ø·Ù„Ù‚! Ù„Ù†Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ

# ğŸ“ Ø£ÙˆÙ„Ø§Ù‹: Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª

# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
!mkdir -p models core data tests
!touch models/__init__.py
!touch core/__init__.py
!touch data/__init__.py

# ğŸ”§ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø®ÙÙŠÙ

# Ø§Ù„Ù…Ù„Ù: models/lightweight_understanding.py
with open('models/lightweight_understanding.py', 'w', encoding='utf-8') as f:
    f.write('''# models/lightweight_understanding.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import re
from pathlib import Path
import json

class LightweightUnderstandingModel:
    def __init__(self, model_path=None):
        self.model_size = "tiny"
        self.max_length = 512
        self.vocab_size = 10000

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.vocab = self.build_arabic_vocab()
        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}

        if model_path and Path(model_path).exists():
            self.load_model(model_path)
        else:
            self.model = self.build_lightweight_model()
            print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ø®ÙÙŠÙ Ù…Ù† Ø§Ù„ØµÙØ±!")

    def build_arabic_vocab(self):
        """Ø¨Ù†Ø§Ø¡ Ù…ÙØ±Ø¯Ø§Øª Ø¹Ø±Ø¨ÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ©"""
        base_vocab = [
            '[PAD]', '[UNK]', '[CLS]', '[SEP]',
            'Ø§Ù„', 'ÙÙŠ', 'Ù…Ù†', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ø£Ù†', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'ÙƒØ§Ù†', 'ÙŠÙƒÙˆÙ†',
            'Ù…Ø§', 'Ù‡Ø°Ø§', 'Ø°Ù„Ùƒ', 'Ù‡Ø°Ù‡', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠÙ†',
            'Ù…Ø¹', 'Ø¨Ø¯ÙˆÙ†', 'Ø­ÙˆÙ„', 'Ø®Ù„Ø§Ù„', 'Ø¨ÙŠÙ†', 'Ø¶Ù…Ù†', 'Ø¹Ù†', 'ÙÙŠÙ…Ø§'
        ]

        # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
        educational_terms = [
            'ØªØ¹Ø±ÙŠÙ', 'Ù…ÙÙ‡ÙˆÙ…', 'Ø´Ø±Ø­', 'ØªØ­Ù„ÙŠÙ„', 'ØªÙ„Ø®ÙŠØµ', 'Ù†ØªÙŠØ¬Ø©', 'Ø®Ù„Ø§ØµØ©',
            'Ø±ÙŠØ§Ø¶ÙŠØ§Øª', 'Ø¹Ù„ÙˆÙ…', 'Ù„ØºØ©', 'ØªØ§Ø±ÙŠØ®', 'Ø¬ØºØ±Ø§ÙÙŠØ§', 'ÙÙŠØ²ÙŠØ§Ø¡', 'ÙƒÙŠÙ…ÙŠØ§Ø¡',
            'Ù…Ø¹Ø§Ø¯Ù„Ø©', 'Ù†Ø¸Ø±ÙŠØ©', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø¨Ø¯Ø£', 'ÙØ±Ø¶ÙŠØ©', 'ØªØ¬Ø±Ø¨Ø©', 'ØªØ­Ù‚Ù‚',
            'Ø£Ù…Ø«Ù„Ø©', 'ØªØ·Ø¨ÙŠÙ‚Ø§Øª', 'ØªÙ…Ø§Ø±ÙŠÙ†', 'Ù…Ø³Ø§Ø¦Ù„', 'Ø­Ù„', 'Ø¥Ø¬Ø§Ø¨Ø©', 'Ø³Ø¤Ø§Ù„'
        ]

        return base_vocab + educational_terms

    def build_lightweight_model(self):
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø®ÙÙŠÙ Ù…Ù† Ø§Ù„ØµÙØ±"""
        class TinyTransformer(nn.Module):
            def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4):
                super().__init__()
                self.d_model = d_model
                self.embedding = nn.Embedding(vocab_size, d_model)
                self.pos_encoding = PositionalEncoding(d_model)

                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model,
                    nhead=nhead,
                    dim_feedforward=512,
                    dropout=0.1,
                    batch_first=True
                )
                self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

                # Ø±Ø£Ø³ Ù„Ù„ØªÙ„Ø®ÙŠØµ
                self.summary_head = nn.Linear(d_model, vocab_size)

            def forward(self, x):
                x = self.embedding(x) * np.sqrt(self.d_model)
                x = self.pos_encoding(x)
                x = self.encoder(x)
                return self.summary_head(x[:, 0, :])  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ Ø±Ù…Ø² Ù„Ù„Ø¥Ø®Ø±Ø§Ø¬

        return TinyTransformer(self.vocab_size)

    def text_to_tokens(self, text):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²"""
        words = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text.lower())
        tokens = [self.word_to_idx.get(word, self.word_to_idx['[UNK]']) for word in words]
        return tokens[:self.max_length]

    def understand_text(self, text):
        """ÙÙ‡Ù… Ø§Ù„Ù†Øµ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        print(f"ğŸ” ÙÙ‡Ù… Ø§Ù„Ù†Øµ: {text[:50]}...")

        if not text.strip():
            return {'main_ideas': [], 'complexity': 'low', 'word_count': 0}

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø©
        sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]
        word_count = len(text.split())

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        main_ideas = self.extract_main_ideas(text)

        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity = self.assess_complexity(text)

        return {
            'main_ideas': main_ideas[:3],  # Ø£ÙˆÙ„ 3 Ø£ÙÙƒØ§Ø± Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø·
            'complexity': complexity,
            'word_count': word_count,
            'sentence_count': len(sentences),
            'estimated_reading_time': max(1, word_count // 200)  # Ø¯Ù‚Ø§Ø¦Ù‚
        }

    def extract_main_ideas(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ©
        keywords = {
            'Ù…ÙÙ‡ÙˆÙ…': 3, 'ØªØ¹Ø±ÙŠÙ': 3, 'Ù†Ø¸Ø±ÙŠØ©': 3, 'Ù‚Ø§Ù†ÙˆÙ†': 3, 'Ù…Ø¨Ø¯Ø£': 3,
            'Ø£Ù‡Ù…ÙŠØ©': 2, 'ÙØ§Ø¦Ø¯Ø©': 2, 'Ù‡Ø¯Ù': 2, 'ØºØ±Ø¶': 2,
            'Ø®ØµØ§Ø¦Øµ': 2, 'ØµÙØ§Øª': 2, 'Ù…Ù…ÙŠØ²Ø§Øª': 2,
            'Ø£Ù…Ø«Ù„Ø©': 1, 'ØªØ·Ø¨ÙŠÙ‚Ø§Øª': 1, 'Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª': 1
        }

        sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]
        scored_sentences = []

        for sentence in sentences:
            score = 0
            words = sentence.split()

            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ø±Ø¬Ø© Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
            if 5 <= len(words) <= 20:
                score += 2

            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ø±Ø¬Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            for keyword, weight in keywords.items():
                if keyword in sentence:
                    score += weight

            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ø±Ø¬Ø© Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø£Ø®ÙŠØ±Ø©
            if sentence == sentences[0] or sentence == sentences[-1]:
                score += 2

            if score > 0:
                scored_sentences.append((sentence, score))

        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        scored_sentences.sort(key=lambda x: x[1], reverse=True)

        return [sentence for sentence, score in scored_sentences[:3]]

    def assess_complexity(self, text):
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Øµ"""
        words = text.split()
        word_count = len(words)

        if word_count < 50:
            return 'Ù…Ù†Ø®ÙØ¶'
        elif word_count < 200:
            return 'Ù…ØªÙˆØ³Ø·'
        else:
            return 'Ø¹Ø§Ù„ÙŠ'

    def summarize(self, text, max_sentences=3):
        """ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¬ÙˆÙ‡Ø±"""
        print(f"ğŸ“„ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ: {text[:50]}...")

        if not text.strip():
            return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù„ØªÙ„Ø®ÙŠØµÙ‡"

        understanding = self.understand_text(text)
        main_ideas = understanding['main_ideas']

        if not main_ideas:
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙÙƒØ§Ø±ØŒ Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ ÙˆØ¢Ø®Ø± Ø¬Ù…Ù„Ø©
            sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]
            if len(sentences) <= max_sentences:
                return text
            else:
                return '. '.join([sentences[0], sentences[-1]]) + '.'

        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ù„Ø®Øµ Ù…Ù† Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        summary = '. '.join(main_ideas[:max_sentences]) + '.'

        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ†Ø³ÙŠÙ‚
        summary = re.sub(r'\\s+', ' ', summary)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        summary = summary.strip()

        return summary

    def save_model(self, path):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        torch.save({
            'model_state': self.model.state_dict(),
            'vocab': self.vocab,
            'config': {
                'vocab_size': self.vocab_size,
                'max_length': self.max_length
            }
        }, path)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {path}")

    def load_model(self, path):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        checkpoint = torch.load(path, map_location='cpu')
        self.vocab = checkpoint['vocab']
        self.vocab_size = len(self.vocab)
        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}

        self.model = self.build_lightweight_model()
        self.model.load_state_dict(checkpoint['model_state'])
        print(f"ğŸ“‚ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {path}")

class PositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ù…ÙˆØ¶Ø¹ÙŠ Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ­ÙˆÙŠÙ„ÙŠ"""
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

# Ø§Ø®ØªØ¨Ø§Ø± ÙÙˆØ±ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
def test_understanding_model():
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ...")

    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = LightweightUnderstandingModel()

    # Ù†ØµÙˆØµ Ø§Ø®ØªØ¨Ø§Ø±ÙŠØ©
    test_texts = [
        "Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§. ÙŠØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø¬Ø¨Ø± Ù…Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„. ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø¬Ø¨Ø± ÙÙŠ Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ù…ÙˆØ² Ù…Ø«Ù„ x Ùˆ y Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„Ø©.",
        "Ø§Ù„Ø®Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ÙŠØ§Ø©. Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ© ØªØªÙƒÙˆÙ† Ù…Ù† Ø®Ù„Ø§ÙŠØ§. Ù‡Ù†Ø§Ùƒ Ù†ÙˆØ¹Ø§Ù† Ø±Ø¦ÙŠØ³ÙŠØ§Ù† Ù…Ù† Ø§Ù„Ø®Ù„Ø§ÙŠØ§: Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø¨Ø¯Ø§Ø¦ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø© Ù…Ø«Ù„ Ø§Ù„Ø¨ÙƒØªÙŠØ±ÙŠØ§ØŒ ÙˆØ§Ù„Ø®Ù„Ø§ÙŠØ§ Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø© Ù…Ø«Ù„ Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ù†Ø¨Ø§Øª ÙˆØ§Ù„Ø­ÙŠÙˆØ§Ù†.",
        "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù„ØºØ© Ø³Ø§Ù…ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©ØŒ ØªØªÙ…ÙŠØ² Ø¨ØºÙ†Ù‰ Ù…ÙØ±Ø¯Ø§ØªÙ‡Ø§ ÙˆØ¯Ù‚Ø© ØªØ¹Ø¨ÙŠØ±Ù‡Ø§. ØªØ­ØªÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù„Ù‰ 28 Ø­Ø±ÙØ§Ù‹ ÙˆØªÙƒØªØ¨ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±."
    ]

    for i, text in enumerate(test_texts, 1):
        print(f"\\n{'='*50}")
        print(f"ğŸ“ Ø§Ù„Ù†Øµ {i}:")
        print(f"{text}")

        # Ø§Ù„ÙÙ‡Ù…
        understanding = model.understand_text(text)
        print(f"\\nğŸ’¡ Ø§Ù„ÙÙ‡Ù…:")
        print(f"â€¢ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {understanding['main_ideas']}")
        print(f"â€¢ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {understanding['complexity']}")
        print(f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {understanding['word_count']}")

        # Ø§Ù„ØªÙ„Ø®ÙŠØµ
        summary = model.summarize(text)
        print(f"\\nğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:")
        print(f"{summary}")

if __name__ == "__main__":
    test_understanding_model()

''')

# ğŸ§ª Ø§Ù„Ø¢Ù† Ù„Ù†Ø®ØªØ¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø©!

# Ø§Ù„Ù…Ù„Ù: test_phase1.py
with open('test_phase1.py', 'w', encoding='utf-8') as f:
    f.write('''# test_phase1.py
#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ
"""

import sys
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.append(str(Path(__file__).parent))

def main():
    print("ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ")
    print("=" * 60)

    try:
        from models.lightweight_understanding import test_understanding_model
        test_understanding_model()

        print("\\nâœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1 Ù…ÙƒØªÙ…Ù„Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        print("\\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
        print("â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ: ~15MB")
        print("â€¢ Ø§Ù„Ø³Ø±Ø¹Ø©: ÙÙˆØ±ÙŠØ© (<1 Ø«Ø§Ù†ÙŠØ©)")
        print("â€¢ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: ~100MB")
        print("â€¢ Ø§Ù„Ø¯Ø¹Ù…: Ø£ÙˆÙÙ„Ø§ÙŠÙ† ÙƒØ§Ù…Ù„")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
        print("\\nğŸ”§ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª torch:")
        print("pip install torch")

if __name__ == "__main__":
    main()
''')

# ğŸ“¦ Ù…Ù„Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø®ÙÙŠÙ

# Ø§Ù„Ù…Ù„Ù: requirements_lightweight.txt
with open('requirements_lightweight.txt', 'w', encoding='utf-8') as f:
    f.write('''torch>=1.9.0
numpy>=1.21.0
scikit-learn>=1.0.0''')

# ğŸ¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙÙˆØ±ÙŠØ©

# Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±

# ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª
!ls -la models/

# Ø´ØºÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
!python test_phase1.py


# Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡Øª Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¹ torch

# ØªØ«Ø¨ÙŠØª torch Ø®ÙÙŠÙ (Ø¨Ø¯ÙˆÙ† CUDA)
!pip install torch --index-url https://download.pytorch.org/whl/cpu

# Ø£Ùˆ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… GPU:
!pip install torch torchvision torchaudio


# Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠ

# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ ÙÙŠ Python Ù…Ø¨Ø§Ø´Ø±Ø©
!python3 -c "from models.lightweight_understanding import LightweightUnderstandingModel; model = LightweightUnderstandingModel(); result = model.understand_text('Ø§Ù„Ø¬Ø¨Ø± ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª'); print('âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©:', result)"

# ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1

# Ø¨Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ù‰:
# ```
# ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ
# ============================================================
# ğŸ” ÙÙ‡Ù… Ø§Ù„Ù†Øµ: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ²...
# ğŸ’¡ Ø§Ù„ÙÙ‡Ù…:
# â€¢ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: ['Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§']
# â€¢ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: Ù…ØªÙˆØ³Ø·
# â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: 25

# ğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:
# Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§.
# ```

# ğŸ‰ ØªÙ‡Ø§Ù†ÙŠÙ†Ø§! Ù„Ù‚Ø¯ Ø£Ù†Ø´Ø£Øª Ø£ÙˆÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ø¹Ø±Ø¨ÙŠ Ø®ÙÙŠÙ!

# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1 Ù…ÙƒØªÙ…Ù„Ø© âœ…

# Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠÙƒ:

# Â· ğŸ§  Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ù†ØµÙŠ Ø¹Ø±Ø¨ÙŠ
# Â· ğŸ“„ Ù†Ø¸Ø§Ù… ØªÙ„Ø®ÙŠØµ Ø°ÙƒÙŠ
# Â· âš¡ Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹
# Â· ğŸ’¾ ÙŠØ¹Ù…Ù„ Ø£ÙˆÙÙ„Ø§ÙŠÙ†

# Ù‡Ù„ ÙŠØ¹Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ØŸ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬Ø§Ù‡Ø²Ø§Ù‹ØŒ ÙÙ„Ù†Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©! ğŸš€ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ© - Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒØ§Ù…Ù„

# ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©

# Ø§Ù„Ù…Ù„Ù: models/smart_question_system.py
with open('models/smart_question_system.py', 'w', encoding='utf-8') as f:
    f.write('''# models/smart_question_system.py
import re
import random
from typing import List, Dict
import numpy as np

class SmartQuestionGenerator:
    def __init__(self):
        # Ù‚ÙˆØ§Ù„Ø¨ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© Ø¨Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ©
        self.question_templates = {
            'comprehension': [
                "Ù…Ø§ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØªØ­Ø¯Ø« Ø¹Ù†Ù‡Ø§ Ø§Ù„Ù†Øµ?",
                "Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ¹Ù†ÙŠÙ‡ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©: '{phrase}' ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹?",
                "Ø¹Ø±Ù/Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… '{concept}' Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ.",
                "Ù…Ø§ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Ø¯Ø±Ø§Ø³Ø© '{concept}'?",
                "Ø§Ø°ÙƒØ± Ø«Ù„Ø§Ø« Ù†Ù‚Ø§Ø· Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ±Ø¯Øª ÙÙŠ Ø§Ù„Ù†Øµ."
            ],
            'application': [
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ '{concept}' ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©?",
                "Ù…Ø§ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… '{concept}' Ù…Ù† ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø­ÙŠØ§Ø©?",
                "ÙƒÙŠÙ ØªØ­Ù„ Ù…Ø´ÙƒÙ„Ø© Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù…Ø´ÙƒÙ„Ø© '{concept}' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø§ ØªØ¹Ù„Ù…ØªÙ‡?",
                "ØµÙ…Ù… ØªØ¬Ø±Ø¨Ø© Ø¨Ø³ÙŠØ·Ø© ØªÙˆØ¶Ø­ '{concept}'.",
                "Ù…Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„ÙÙƒØ±Ø© '{concept}'?"
            ],
            'analysis': [
                "Ù…Ø§ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù Ø¨ÙŠÙ† '{concept1}' Ùˆ '{concept2}'?",
                "Ù„Ù…Ø§Ø°Ø§ ÙŠØ¹ØªØ¨Ø± '{concept}' Ù…Ù‡Ù…Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ?",
                "Ù…Ø§ Ø§Ù„Ø¢Ø«Ø§Ø±/Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªØ±ØªØ¨Ø© Ø¹Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ '{action}'?",
                "Ø­Ù„Ù„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† '{concept1}' Ùˆ '{concept2}'.",
                "Ù…Ø§ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ØªÙŠ Ø£Ø¯Øª Ø¥Ù„Ù‰ '{result}' Ø­Ø³Ø¨ Ø§Ù„Ù†Øµ?"
            ],
            'evaluation': [
                "Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ© '{concept}' Ù„Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª?",
                "Ù‚ÙŠÙ… Ø£Ù‡Ù…ÙŠØ© '{concept}' ÙÙŠ Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ø¹Ù„Ù…ÙŠ.",
                "Ù…Ø§ Ù…Ø¯Ù‰ ØµØ­Ø© Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©: '{phrase}'? Ø¨Ø±Ø± Ø¥Ø¬Ø§Ø¨ØªÙƒ.",
                "Ù…Ø§ Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ù„Ù€ '{concept}'?",
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† ØªØ·Ø¨ÙŠÙ‚ '{concept}'?"
            ],
            'synthesis': [
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬ '{concept1}' Ù…Ø¹ '{concept2}' Ù„Ø¥Ù†ØªØ§Ø¬ ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©?",
                "Ù…Ø§ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ù„Ø§ØµÙ‡Ø§ Ù…Ù† Ø§Ù„Ù†Øµ?",
                "ØµÙ…Ù… Ø®Ø·Ø© Ø¨Ø­Ø«ÙŠØ© Ù„Ø¯Ø±Ø§Ø³Ø© '{concept}'.",
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ± '{concept}' Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹?",
                "Ù…Ø§ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø¨ØªÙƒØ±Ø© Ù„Ù…Ø´ÙƒÙ„Ø© '{problem}'?"
            ]
        }

        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†Ù‡Ø§
        self.edu_keywords = {
            'ØªØ¹Ø±ÙŠÙ': 3, 'Ù…ÙÙ‡ÙˆÙ…': 3, 'Ù†Ø¸Ø±ÙŠØ©': 3, 'Ù‚Ø§Ù†ÙˆÙ†': 3, 'Ù…Ø¨Ø¯Ø£': 3,
            'Ø®ØµØ§Ø¦Øµ': 2, 'ØµÙØ§Øª': 2, 'Ù…Ù…ÙŠØ²Ø§Øª': 2, 'Ø£Ù†ÙˆØ§Ø¹': 2, 'Ø£Ù‚Ø³Ø§Ù…': 2,
            'Ø£Ù‡Ù…ÙŠØ©': 2, 'ÙØ§Ø¦Ø¯Ø©': 2, 'Ù‡Ø¯Ù': 2, 'ØºØ±Ø¶': 2,
            'Ø£Ù…Ø«Ù„Ø©': 1, 'ØªØ·Ø¨ÙŠÙ‚Ø§Øª': 1, 'Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª': 1, 'Ù†Ù…Ø§Ø°Ø¬': 1,
            'Ù†ØªÙŠØ¬Ø©': 2, 'Ø®Ù„Ø§ØµØ©': 2, 'Ø§Ø³ØªÙ†ØªØ§Ø¬': 2, 'ØªØ­Ù„ÙŠÙ„': 2
        }

        # Ù…ØµØ·Ù„Ø­Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
        self.common_concepts = [
            'Ø§Ù„Ø¬Ø¨Ø±', 'Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©', 'Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª', 'Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª', 'Ø§Ù„Ø¯ÙˆØ§Ù„',
            'Ø§Ù„Ø®Ù„ÙŠØ©', 'Ø§Ù„ØªÙƒØ§Ø«Ø±', 'Ø§Ù„ÙˆØ±Ø§Ø«Ø©', 'Ø§Ù„ØªØ·ÙˆØ±', 'Ø§Ù„Ø¨ÙŠØ¦Ø©',
            'Ø§Ù„Ù‚ÙˆØ©', 'Ø§Ù„Ø·Ø§Ù‚Ø©', 'Ø§Ù„Ø­Ø±ÙƒØ©', 'Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡', 'Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©',
            'Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª', 'Ø§Ù„Ù…Ø±ÙƒØ¨Ø§Øª', 'Ø§Ù„Ø¹Ù†Ø§ØµØ±', 'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¯ÙˆØ±ÙŠ'
        ]

    def generate_questions(self, text: str, num_questions: int = 3, question_types: List[str] = None) -> List[Dict]:
        """ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ"""

        print(f"ğŸ§  ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ù†Øµ Ù…ÙƒÙˆÙ† Ù…Ù† {len(text.split())} ÙƒÙ„Ù…Ø©...")

        if not text.strip():
            return [{
                'question': "Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø¯Ù… ÙØ§Ø±Øº. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ù†Øµ Ù„Ù„Ù…Ù†Ø§Ù‚Ø´Ø©?",
                'type': 'general',
                'difficulty': 'low',
                'concepts_used': []
            }]

        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        concepts = self.extract_concepts(text)
        phrases = self.extract_key_phrases(text)

        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù…ÙØ§Ù‡ÙŠÙ…ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù…ØµØ·Ù„Ø­Ø§Øª Ø¹Ø§Ù…Ø©
        if not concepts:
            concepts = self.common_concepts[:3]

        if not concepts and not phrases:
            return [{
                'question': "Ø§Ù„Ù†Øµ Ø¨Ø³ÙŠØ·. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù„Ø´Ø±Ø­ Ø£Ùˆ ØªÙ„Ø®ÙŠØµ Ø¥Ø¶Ø§ÙÙŠ?",
                'type': 'general',
                'difficulty': 'low',
                'concepts_used': []
            }]

        # ØªØ­Ø¯ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        if question_types is None:
            question_types = ['comprehension', 'application', 'analysis']

        questions = []
        attempts = 0
        max_attempts = num_questions * 3

        while len(questions) < num_questions and attempts < max_attempts:
            attempts += 1

            # Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            q_type = random.choice(question_types)
            template = random.choice(self.question_templates[q_type])

            question_text = template
            used_concepts = []

            try:
                # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø¨Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…/Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
                if '{concept}' in template and concepts:
                    concept = random.choice(concepts)
                    question_text = template.format(concept=concept)
                    used_concepts.append(concept)

                elif '{phrase}' in template and phrases:
                    phrase = random.choice(phrases)
                    question_text = template.format(phrase=phrase)
                    used_concepts.append(phrase)

                elif '{concept1}' in template and '{concept2}' in template and len(concepts) >= 2:
                    concept1, concept2 = random.sample(concepts, 2)
                    question_text = template.format(concept1=concept1, concept2=concept2)
                    used_concepts.extend([concept1, concept2])

                elif '{action}' in template and concepts:
                    action = random.choice(concepts)
                    question_text = template.format(action=action)
                    used_concepts.append(action)

                elif '{result}' in template and concepts:
                    result = random.choice(concepts)
                    question_text = template.format(result=result)
                    used_concepts.append(result)

                elif '{problem}' in template and concepts:
                    problem = random.choice(concepts)
                    question_text = template.format(problem=problem)
                    used_concepts.append(problem)

                # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ù…Ù„Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙÙ‡ÙˆÙ… Ø¹Ø´ÙˆØ§Ø¦ÙŠ
                if ('{' in question_text and '}' in question_text) and concepts:
                    concept = random.choice(concepts)
                    question_text = question_text.format(
                        concept=concept,
                        concept1=concept,
                        concept2=concept,
                        phrase=phrases[0] if phrases else "Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø·Ø©",
                        action=concept,
                        result=concept,
                        problem=concept
                    )
                    used_concepts.append(concept)

                # ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø©
                if not any(q['question'] == question_text for q in questions):
                    questions.append({
                        'question': question_text,
                        'type': q_type,
                        'difficulty': self.assess_difficulty(q_type, len(used_concepts)),
                        'concepts_used': used_concepts
                    })

            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„: {e}")
                continue

        # Ø¥Ø°Ø§ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ØŒ Ù†Ø¶ÙŠÙ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø©
        if len(questions) < num_questions:
            general_questions = [
                "Ù…Ø§ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ?",
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙÙŠ Ø§Ù„Ø¯Ø±Ø§Ø³Ø©?",
                "Ù…Ø§ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø§Ù„ØªÙŠ ØªÙˆØ¯ Ù…Ø¹Ø±ÙØªÙ‡Ø§ Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹?"
            ]

            for gq in general_questions[:num_questions - len(questions)]:
                questions.append({
                    'question': gq,
                    'type': 'general',
                    'difficulty': 'medium',
                    'concepts_used': []
                })

        return questions[:num_questions]

    def extract_concepts(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©"""
        concepts = []
        sentences = re.split(r'[.!ØŸ]', text)

        for sentence in sentences:
            words = sentence.split()
            for i, word in enumerate(words):
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
                if word in self.edu_keywords:
                    # Ù†Ø£Ø®Ø° Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙƒÙ…ÙØ§Ù‡ÙŠÙ… Ù…Ø­ØªÙ…Ù„Ø©
                    potential_concepts = []

                    # Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø©
                    if i + 1 < len(words):
                        next_word = words[i + 1].strip(' ,:;-')
                        if len(next_word) > 2 and not next_word.isdigit():
                            potential_concepts.append(next_word)

                    # Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ù„Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„ "Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø¬Ø¨Ø±")
                    if i > 0:
                        prev_word = words[i - 1].strip(' ,:;-')
                        if len(prev_word) > 2 and not prev_word.isdigit():
                            potential_concepts.append(prev_word)

                    concepts.extend(potential_concepts)

        # ØªÙ†Ø¸ÙŠÙ ÙˆØªØµÙÙŠØ© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        cleaned_concepts = []
        for concept in concepts:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
            common_words = {'Ù‡Ùˆ', 'ÙÙŠ', 'Ù…Ù†', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ø£Ù†', 'ÙƒØ§Ù†'}
            if (
                concept not in common_words and
                len(concept) > 2 and
                not concept.isdigit()
            ):
                cleaned_concepts.append(concept)

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        seen = set()
        unique_concepts = []
        for concept in cleaned_concepts:
            if concept not in seen:
                seen.add(concept)
                unique_concepts.append(concept)

        return unique_concepts[:8]  # 8 Ù…ÙØ§Ù‡ÙŠÙ… ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰

    def extract_key_phrases(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ù†Øµ"""
        sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]

        if not sentences:
            return []

        key_phrases = []

        # Ø£ÙˆÙ„ Ø¬Ù…Ù„Ø© (ØºØ§Ù„Ø¨Ø§Ù‹ ØªÙƒÙˆÙ† ØªÙ…Ù‡ÙŠØ¯ÙŠØ©)
        if sentences:
            key_phrases.append(sentences[0])

        # Ø¢Ø®Ø± Ø¬Ù…Ù„Ø© (ØºØ§Ù„Ø¨Ø§Ù‹ ØªÙƒÙˆÙ† Ø®Ù„Ø§ØµØ©)
        if len(sentences) > 1:
            key_phrases.append(sentences[-1])

        # Ø£Ø·ÙˆÙ„ Ø¬Ù…Ù„Ø© (Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ù…Ù‡Ù…Ø©)
        if len(sentences) > 2:
            longest_sentence = max(sentences[1:-1] if len(sentences) > 2 else sentences, key=len)
            if longest_sentence not in key_phrases:
                key_phrases.append(longest_sentence)

        # Ø¬Ù…Ù„ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
        for sentence in sentences:
            if any(keyword in sentence for keyword in self.edu_keywords):
                if sentence not in key_phrases and len(key_phrases) < 5:
                    key_phrases.append(sentence)

        return key_phrases[:4]

    def assess_difficulty(self, question_type: str, num_concepts: int) -> str:
        """ØªÙ‚ÙŠÙŠÙ… ØµØ¹ÙˆØ¨Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹Ù‡ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…"""
        base_difficulty = {
            'comprehension': 'low',
            'application': 'medium',
            'analysis': 'high',
            'evaluation': 'high',
            'synthesis': 'high',
            'general': 'medium'
        }

        difficulty = base_difficulty.get(question_type, 'medium')

        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØµØ¹ÙˆØ¨Ø© Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        if num_concepts >= 2 and difficulty == 'medium':
            return 'high'
        elif num_concepts >= 3:
            return 'high'

        return difficulty

    def generate_questions_by_type(self, text: str, question_type: str, num_questions: int = 2) -> List[Dict]:
        """ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ù†ÙˆØ¹ Ù…Ø­Ø¯Ø¯"""
        return self.generate_questions(text, num_questions, [question_type])

    def get_question_statistics(self, questions: List[Dict]) -> Dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø©"""
        type_count = {}
        difficulty_count = {}
        all_concepts = []

        for q in questions:
            # Ø¹Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            type_count[q['type']] = type_count.get(q['type'], 0) + 1

            # Ø¹Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØµØ¹ÙˆØ¨Ø©
            difficulty_count[q['difficulty']] = difficulty_count.get(q['difficulty'], 0) + 1

            # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
            all_concepts.extend(q['concepts_used'])

        # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        unique_concepts = list(set(all_concepts))

        return {
            'total_questions': len(questions),
            'type_distribution': type_count,
            'difficulty_distribution': difficulty_count,
            'unique_concepts_used': unique_concepts,
            'total_concepts_used': len(all_concepts)
        }

# Ø§Ø®ØªØ¨Ø§Ø± ÙÙˆØ±ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…
def test_question_generator():
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©...")
    print("=" * 60)

    generator = SmartQuestionGenerator()

    test_text = (
        "Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§ØªØŒ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„. "
        "Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ù…ØªØºÙŠØ± Ù‡Ùˆ Ø±Ù…Ø² (Ø¹Ø§Ø¯Ø©Ù‹ x Ø£Ùˆ y) ÙŠÙ…Ø«Ù„ Ù‚ÙŠÙ…Ø© Ù…Ø¬Ù‡ÙˆÙ„Ø©. "
        "Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø¬Ø¨Ø± ØªÙƒÙ…Ù† ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ‡ Ø§Ù„ÙˆØ§Ø³Ø¹Ø© ÙÙŠ Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø© ÙˆØ§Ù„ØªÙ…ÙˆÙŠÙ„ØŒ ÙˆÙ‡Ùˆ Ù…Ø¨Ø¯Ø£ Ø£Ø³Ø§Ø³ÙŠ Ù„ÙÙ‡Ù… Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡. "
        "ØªØªØ¶Ù…Ù† Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø¬Ø¨Ø±ÙŠØ© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¬Ù…Ø¹ ÙˆØ§Ù„Ø·Ø±Ø­ ÙˆØ§Ù„Ø¶Ø±Ø¨ Ø¹Ù„Ù‰ Ø·Ø±ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø²Ù†. "
        "Ù†Ø¸Ø±ÙŠØ© ÙÙŠØ«Ø§ØºÙˆØ±Ø³ Ù‡ÙŠ Ø£Ø­Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© Ù„Ù„Ø¬Ø¨Ø± ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©."
    )

    print("\\nğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ¯Ø±:")
    print(test_text)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
    concepts = generator.extract_concepts(test_text)
    print(f"\\nğŸ’¡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {concepts}")

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    phrases = generator.extract_key_phrases(test_text)
    print(f"ğŸ“Œ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {phrases}")

    # ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©
    print(f"\\nâ“ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø© (5 Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©):")
    questions = generator.generate_questions(test_text, 5)

    for i, q in enumerate(questions, 1):
        print(f"  {i}. [{q['type'].upper()}] - Ù…Ø³ØªÙˆÙ‰: {q['difficulty']}")
        print(f"     Ø§Ù„Ø³Ø¤Ø§Ù„: {q['question']}")
        if q['concepts_used']:
            print(f"     Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {', '.join(q['concepts_used'])}")
        print()

    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = generator.get_question_statistics(questions)
    print(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:")
    print(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: {stats['total_questions']}")
    print(f"   â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹: {stats['type_distribution']}")
    print(f"   â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØµØ¹ÙˆØ¨Ø©: {stats['difficulty_distribution']}")
    print(f"   â€¢ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(stats['unique_concepts_used'])} Ù…ÙÙ‡ÙˆÙ…")

    # Ø§Ø®ØªØ¨Ø§Ø± ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
    print(f"\\nğŸ¯ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙ‚Ø· (2 Ø³Ø¤Ø§Ù„):")
    analysis_questions = generator.generate_questions_by_type(test_text, 'analysis', 2)
    for i, q in enumerate(analysis_questions, 1):
        print(f"  {i}. {q['question']}")

if __name__ == "__main__":
    test_question_generator()
''')

# ğŸ§ª Ù…Ù„Ù Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2

# Ø§Ù„Ù…Ù„Ù: test_phase2.py
with open('test_phase2.py', 'w', encoding='utf-8') as f:
    f.write('''# test_phase2.py
#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©
"""

import sys
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.append(str(Path(__file__).parent))

def main():
    print("ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©")
    print("=" * 60)

    try:
        # Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
        from models.smart_question_system import test_question_generator
        test_question_generator()

        print("\\nâœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ù…ÙƒØªÙ…Ù„Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        print("\\nğŸ¯ Ù…Ù…ÙŠØ²Ø§Øª Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:")
        print("â€¢ 5 Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© (ÙÙ‡Ù…ØŒ ØªØ·Ø¨ÙŠÙ‚ØŒ ØªØ­Ù„ÙŠÙ„ØŒ ØªÙ‚ÙŠÙŠÙ…ØŒ ØªØ±ÙƒÙŠØ¨)")
        print("â€¢ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª")
        print("â€¢ ØªÙ‚ÙŠÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©")
        print("â€¢ Ù…Ù†Ø¹ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø³Ø¦Ù„Ø©")
        print("â€¢ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©")

        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1
        print("\\nğŸ”— Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1:")
        from models.lightweight_understanding import LightweightUnderstandingModel
        from models.smart_question_system import SmartQuestionGenerator

        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù…
        understanding_model = LightweightUnderstandingModel()
        # Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        question_generator = SmartQuestionGenerator()

        test_text = "Ø¹Ù„Ù… Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ ÙŠØ¯Ø±Ø³ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ© ÙˆØ·Ø±Ù‚ ØªÙƒØ§Ø«Ø±Ù‡Ø§ ÙˆØªØ·ÙˆØ±Ù‡Ø§. Ø§Ù„Ø®Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ÙŠØ§Ø©."

        print(f"ğŸ“ Ø§Ù„Ù†Øµ: {test_text}")

        # Ø§Ù„ÙÙ‡Ù… Ø£ÙˆÙ„Ø§Ù‹
        understanding = understanding_model.understand_text(test_text)
        print(f"ğŸ’¡ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {understanding['main_ideas']}")

        # Ø«Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        questions = question_generator.generate_questions(test_text, 3)
        print(f"â“ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø©:")
        for i, q in enumerate(questions, 1):
            print(f"  {i}. {q['question']}")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
''')

# ğŸ¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙˆØ±ÙŠØ©

# Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2
!python test_phase2.py


# Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹

# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ ÙÙŠ Python Ù…Ø¨Ø§Ø´Ø±Ø©
!python3 -c "from models.smart_question_system import SmartQuestionGenerator; generator = SmartQuestionGenerator(); text = 'Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø¹Ù„Ù… ÙŠØ¯Ø±Ø³ Ø§Ù„ÙƒÙ… ÙˆØ§Ù„Ø¹Ø¯Ø¯ ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø©. Ø§Ù„Ø¬Ø¨Ø± ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª.'; questions = generator.generate_questions(text, 3); for i, q in enumerate(questions, 1): print(f'{i}. {q[\"question\"]}')"

# ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2

# Ø¨Ø¹Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ù‰:
# ```
# ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©
# ============================================================
# ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ¯Ø±: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§ØªØŒ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª...

# ğŸ’¡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: ['Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª', 'Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª', 'Ø§Ù„Ø¯ÙˆØ§Ù„', 'Ø§Ù„Ø¬Ø¨Ø±']

# â“ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø© (5 Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©):
#   1. [COMPREHENSION] - Ù…Ø³ØªÙˆÙ‰: medium
#      Ø§Ù„Ø³Ø¤Ø§Ù„: Ù…Ø§ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØªØ­Ø¯Ø« Ø¹Ù†Ù‡Ø§ Ø§Ù„Ù†ØµØŸ
#      Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: []

#   2. [APPLICATION] - Ù…Ø³ØªÙˆÙ‰: medium
#      Ø§Ù„Ø³Ø¤Ø§Ù„: ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¬Ø¨Ø± ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŸ
#      Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: ['Ø§Ù„Ø¬Ø¨Ø±']

#   3. [ANALYSIS] - Ù…Ø³ØªÙˆÙ‰: high
#      Ø§Ù„Ø³Ø¤Ø§Ù„: Ù…Ø§ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§ØªØŸ
#      Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: ['Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª', 'Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª']
# ```

# ğŸ‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ù…ÙƒØªÙ…Ù„Ø©! âœ…

# Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠÙƒ Ù†Ø¸Ø§Ù… Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠ Ù…ØªÙƒØ§Ù…Ù„ ÙŠØªÙ…ÙŠØ² Ø¨Ù€:

# Â· ğŸ¯ 5 Ù…Ø³ØªÙˆÙŠØ§Øª ØªÙÙƒÙŠØ± (Ø¨Ù„ÙˆÙ… Ø§Ù„ØªØµÙ†ÙŠÙÙŠ)
# Â· ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª
# Â· ğŸ“Š ØªÙ‚ÙŠÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©
# Â· ğŸš€ ØªÙƒØ§Ù…Ù„ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù…
# Â· ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©

# Ù‡Ù„ ÙŠØ¹Ù…Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ØŸ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬Ø§Ù‡Ø²Ø§Ù‹ØŒ ÙÙ„Ù†Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„ÙƒØ§Ù…Ù„! ğŸš€ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø®ÙÙŠÙ - Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

# ğŸ“ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø³Ù†: models/lightweight_smart_ai.py
with open('models/lightweight_smart_ai.py', 'w', encoding='utf-8') as f:
    f.write('''# models/lightweight_smart_ai.py
import os
import json
import re
import time
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime, timedelta

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù†Ø¬Ø²Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
from models.lightweight_understanding import LightweightUnderstandingModel
from models.smart_question_system import SmartQuestionGenerator

class LightweightSmartAI:
    def __init__(self, knowledge_base_path=None, enable_spaced_repetition=True):
        print("ğŸ§  ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")

        # 1. ØªØ­Ù…ÙŠÙ„ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø®ÙÙŠÙ
        self.understanding_model = LightweightUnderstandingModel()
        self.question_generator = SmartQuestionGenerator()

        # 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.conversation_memory = []
        self.user_progress = {}
        self.spaced_repetition_enabled = enable_spaced_repetition
        self.knowledge_base = self.load_enhanced_knowledge_base(knowledge_base_path)

        # 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªØ¨Ø§Ø¹Ø¯Ø©
        self.review_schedule = {}
        self.concept_mastery = {}

        print("âœ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„! (Ø£ÙˆÙÙ„Ø§ÙŠÙ† + Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…ØªØ¨Ø§Ø¹Ø¯Ø©)")

    def load_enhanced_knowledge_base(self, path):
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù…ÙˆØ³Ø¹Ø© ÙˆÙ…Ø­Ø³Ù†Ø©"""
        if path and Path(path).exists():
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)

        # Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù…ÙˆØ³Ø¹Ø© Ù…Ø¹ ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©
        return {
            "Ø§Ù„Ø¬Ø¨Ø±": {
                "definition": "Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§. ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„.",
                "importance": "Ø£Ø³Ø§Ø³ÙŠ ÙÙŠ Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø© ÙˆØ§Ù„ØªÙ…ÙˆÙŠÙ„.",
                "examples": ["Ø­Ù„ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø®Ø·ÙŠØ©", "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙˆØ§Ù„"]
            },
            "Ø§Ù„Ø®Ù„ÙŠØ©": {
                "definition": "Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ÙŠØ§Ø©. ØªØªÙƒÙˆÙ† Ù…Ù†Ù‡Ø§ Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ©.",
                "types": ["Ø¨Ø¯Ø§Ø¦ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø©", "Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø©"],
                "analogy": "Ù…ØµÙ†Ø¹ ØµØºÙŠØ± ÙŠÙ‚ÙˆÙ… Ø¨Ø¬Ù…ÙŠØ¹ ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø­ÙŠØ§Ø©."
            },
            "Ù†Ø¸Ø±ÙŠØ© ÙÙŠØ«Ø§ØºÙˆØ±Ø³": {
                "definition": "ÙÙŠ Ø§Ù„Ù…Ø«Ù„Ø« Ø§Ù„Ù‚Ø§Ø¦Ù… Ø§Ù„Ø²Ø§ÙˆÙŠØ©ØŒ Ù…Ø±Ø¨Ø¹ Ø·ÙˆÙ„ Ø§Ù„ÙˆØªØ± ÙŠØ³Ø§ÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹ Ù…Ø±Ø¨Ø¹ÙŠ Ø·ÙˆÙ„ÙŠ Ø§Ù„Ø¶Ù„Ø¹ÙŠÙ† Ø§Ù„Ø¢Ø®Ø±ÙŠÙ† (aÂ² + bÂ² = cÂ²).",
                "application": "Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§ÙØ§ØªØŒ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©.",
                "related_concepts": ["Ø§Ù„Ù…Ø«Ù„Ø« Ø§Ù„Ù‚Ø§Ø¦Ù…", "Ø§Ù„ÙˆØªØ±", "Ø§Ù„Ø£Ø¶Ù„Ø§Ø¹"]
            }
        }

    def process_text(self, text: str) -> Dict[str, Any]:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ Ù„ÙÙ‡Ù…Ù‡ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©"""
        print(f"âœ¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ: {text[:50]}...")

        understanding = self.understanding_model.understand_text(text)
        questions = self.question_generator.generate_questions(text, num_questions=3)

        return {
            'understanding': understanding,
            'generated_questions': questions,
            'summary': self.understanding_model.summarize(text)
        }

    def query_knowledge_base(self, concept: str) -> Dict[str, Any]:
        """Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø¹Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        concept_data = self.knowledge_base.get(concept, {})
        if concept_data:
            print(f"ğŸ“š Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø¹Ù†: {concept}")
        else:
            print(f"ğŸ¤·â€â™‚ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙØµÙ„Ø© Ø¹Ù† '{concept}' ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©.")
        return concept_data

    def get_contextual_response(self, user_query: str) -> str:
        """ÙŠÙ‚Ø¯Ù… Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø³ÙŠØ§Ù‚ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…"""
        user_query_lower = user_query.lower()

        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø£ÙˆÙ„Ø§Ù‹
        for concept, data in self.knowledge_base.items():
            if concept.lower() in user_query_lower:
                definition = data.get("definition", "Ù„Ø§ ÙŠÙˆØ¬Ø¯ ØªØ¹Ø±ÙŠÙ Ù…Ø­Ø¯Ø¯.")
                return f"Ø­Ø³Ø¨ Ù…Ø§ Ø£Ø¹Ø±ÙÙ‡ØŒ {concept} ØªØ¹Ù†ÙŠ: {definition}"

        # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŒ Ø­Ø§ÙˆÙ„ Ø§Ù„ÙÙ‡Ù… ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø©
        understanding = self.understanding_model.understand_text(user_query)
        if understanding['main_ideas']:
            return f"ÙÙ‡Ù…Øª Ø£Ù† Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙŠ Ø³Ø¤Ø§Ù„Ùƒ Ù‡ÙŠ: {understanding['main_ideas'][0]}." \
                   " Ù‡Ù„ ØªÙˆØ¯ Ø£Ù† Ø£Ø³Ø£Ù„Ùƒ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¹Ù†Ù‡Ø§ØŸ"
        else:
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ÙÙ‡Ù… Ø³Ø¤Ø§Ù„Ùƒ ØªÙ…Ø§Ù…Ø§Ù‹. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØµÙŠØ§ØºØ©ØŸ"

    def update_user_progress(self, user_id: str, concept: str, mastered: bool):
        """ØªØ­Ø¯ÙŠØ« ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø¥ØªÙ‚Ø§Ù† Ù…ÙÙ‡ÙˆÙ… Ù…Ø¹ÙŠÙ†"""
        if user_id not in self.user_progress:
            self.user_progress[user_id] = {}
        self.user_progress[user_id][concept] = {'mastered': mastered, 'last_reviewed': datetime.now().isoformat()}
        print(f"âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« ØªÙ‚Ø¯Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} Ù„Ù„Ù…ÙÙ‡ÙˆÙ… '{concept}'.")

    def schedule_review(self, user_id: str, concept: str, current_interval: int = 1):
        """Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªØ¨Ø§Ø¹Ø¯Ø© Ù„Ù…ÙÙ‡ÙˆÙ… Ù…Ø¹ÙŠÙ†"""
        if not self.spaced_repetition_enabled:
            return

        next_review_date = datetime.now() + timedelta(days=current_interval)
        if user_id not in self.review_schedule:
            self.review_schedule[user_id] = {}
        self.review_schedule[user_id][concept] = {'next_review': next_review_date.isoformat(), 'interval': current_interval * 2}
        print(f"ğŸ—“ï¸ ØªÙ… Ø¬Ø¯ÙˆÙ„Ø© Ù…Ø±Ø§Ø¬Ø¹Ø© '{concept}' Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id} ÙÙŠ: {next_review_date.date()}")

    def get_due_reviews(self, user_id: str) -> List[str]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„ØªÙŠ Ø­Ø§Ù† ÙˆÙ‚Øª Ù…Ø±Ø§Ø¬Ø¹ØªÙ‡Ø§"""
        if user_id not in self.review_schedule or not self.spaced_repetition_enabled:
            return []

        due_concepts = []
        for concept, details in self.review_schedule[user_id].items():
            if datetime.fromisoformat(details['next_review']) <= datetime.now():
                due_concepts.append(concept)
        return due_concepts

    def simulate_conversation(self, user_id: str, texts: List[str]):
        """Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        print(f"--- Ø¨Ø¯Ø¡ Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_id} ---")
        for i, text in enumerate(texts):
            print(f"\n>> Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ({user_id}): {text}")

            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ
            processed_data = self.process_text(text)

            # Ø§Ù„Ø±Ø¯ÙˆØ¯
            print(f"<< Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: ÙÙ‡Ù…Øª Ø§Ù„Ù†Øµ. Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {processed_data['understanding']['main_ideas']}")
            if processed_data['generated_questions']:
                print("<< Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: Ø¥Ù„ÙŠÙƒ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:")
                for q in processed_data['generated_questions']:
                    print(f"   - {q['question']}")

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù… ÙˆØ¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© (Ù…Ø«Ø§Ù„ Ù…Ø¨Ø³Ø·)
            if processed_data['understanding']['main_ideas']:
                concept = processed_data['understanding']['main_ideas'][0].split()[0] # Ø£ÙˆÙ„ ÙƒÙ„Ù…Ø© ÙƒÙ€ Ù…ÙÙ‡ÙˆÙ…
                self.update_user_progress(user_id, concept, True)
                self.schedule_review(user_id, concept)

            # Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
            response = self.get_contextual_response(text)
            print(f"<< Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (Ø³ÙŠØ§Ù‚ÙŠ): {response}")

            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø©
            due_reviews = self.get_due_reviews(user_id)
            if due_reviews:
                print(f"<< Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ: Ø­Ø§Ù† ÙˆÙ‚Øª Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¨Ø¹Ø¶ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {', '.join(due_reviews)}")

            time.sleep(1) # Ù…Ø­Ø§ÙƒØ§Ø© ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©

        print(f"\n--- Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…: {user_id} ---")

# Ø§Ø®ØªØ¨Ø§Ø± ÙÙˆØ±ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„
def test_integrated_ai():
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø®ÙÙŠÙ...")
    print("=" * 60)

    ai_system = LightweightSmartAI()

    # Ù†Øµ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_text_1 = "Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§. ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø¬Ø¨Ø± ÙÙŠ Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª."
    test_text_2 = "Ø§Ù„Ø®Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ÙŠØ§Ø©. Ù‡Ù†Ø§Ùƒ Ù†ÙˆØ¹Ø§Ù† Ø±Ø¦ÙŠØ³ÙŠØ§Ù†: Ø¨Ø¯Ø§Ø¦ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø© ÙˆØ­Ù‚ÙŠÙ‚ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø©."
    test_text_3 = "Ù†Ø¸Ø±ÙŠØ© ÙÙŠØ«Ø§ØºÙˆØ±Ø³ ØªÙ†Øµ Ø¹Ù„Ù‰ Ø£Ù† ÙÙŠ Ø§Ù„Ù…Ø«Ù„Ø« Ø§Ù„Ù‚Ø§Ø¦Ù…ØŒ Ù…Ø±Ø¨Ø¹ Ø§Ù„ÙˆØªØ± ÙŠØ³Ø§ÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹ Ù…Ø±Ø¨Ø¹ÙŠ Ø§Ù„Ø¶Ù„Ø¹ÙŠÙ† Ø§Ù„Ø¢Ø®Ø±ÙŠÙ†."

    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
    print("\n--- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ---")
    result_1 = ai_system.process_text(test_text_1)
    print(f"\nÙÙ‡Ù… Ø§Ù„Ù†Øµ 1: {result_1['understanding']['main_ideas']}")
    print(f"Ù…Ù„Ø®Øµ Ø§Ù„Ù†Øµ 1: {result_1['summary']}")
    print(f"Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù†Øµ 1: {[q['question'] for q in result_1['generated_questions']]}")

    result_2 = ai_system.process_text(test_text_2)
    print(f"\nÙÙ‡Ù… Ø§Ù„Ù†Øµ 2: {result_2['understanding']['main_ideas']}")
    print(f"Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù†Øµ 2: {[q['question'] for q in result_2['generated_questions']]}")

    # Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©
    print("\n--- Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ---")
    algebra_info = ai_system.query_knowledge_base("Ø§Ù„Ø¬Ø¨Ø±")
    print(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø¬Ø¨Ø±: {algebra_info.get('definition')}")
    cell_info = ai_system.query_knowledge_base("Ø§Ù„Ø®Ù„ÙŠØ©")
    print(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„Ø®Ù„ÙŠØ©: {cell_info.get('definition')}")
    unknown_info = ai_system.query_knowledge_base("Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¡")
    print(f"Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¹Ù† Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¡: {unknown_info}")

    # Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
    print("\n--- Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ© ---")
    response_query_1 = ai_system.get_contextual_response("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¬Ø¨Ø±ØŸ")
    print(f"Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response_query_1}")
    response_query_2 = ai_system.get_contextual_response("Ù…Ø§ Ù‡ÙŠ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø®Ù„Ø§ÙŠØ§ØŸ")
    print(f"Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response_query_2}")
    response_query_3 = ai_system.get_contextual_response("Ø£Ø±ÙŠØ¯ Ø£Ù† Ø£ØªØ¹Ù„Ù… Ø¹Ù† Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡")
    print(f"Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response_query_3}")

    # Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ù…Ø³ØªØ®Ø¯Ù…
    print("\n--- Ù…Ø­Ø§ÙƒØ§Ø© Ù…Ø­Ø§Ø¯Ø«Ø© Ù…Ø¹ Ù…Ø³ØªØ®Ø¯Ù… Ø§ÙØªØ±Ø§Ø¶ÙŠ ---")
    user_conversation = [
        "Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø¬Ø¨Ø± ÙˆÙ…Ø§ Ø£Ù‡Ù…ÙŠØªÙ‡ØŸ",
        "Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø®Ù„ÙŠØ© Ø¨Ø§Ù„ØªÙØµÙŠÙ„.",
        "Ù…Ø§Ø°Ø§ Ø¹Ù† Ù†Ø¸Ø±ÙŠØ© ÙÙŠØ«Ø§ØºÙˆØ±Ø³ØŸ"
    ]
    ai_system.simulate_conversation("student_1", user_conversation)

    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø©
    print("\n--- Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø§Øª Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø© ---")
    due_reviews_after_conv = ai_system.get_due_reviews("student_1")
    print(f"Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ­Ù‚Ø© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©: {due_reviews_after_conv}")

if __name__ == "__main__":
    test_integrated_ai()

''')

# ğŸ§ª Ù…Ù„Ù Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3

# Ø§Ù„Ù…Ù„Ù: test_phase3.py
with open('test_phase3.py', 'w', encoding='utf-8') as f:
    f.write('''# test_phase3.py
#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø®ÙÙŠÙ
"""

import sys
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.append(str(Path(__file__).parent))

def main():
    print("ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø®ÙÙŠÙ")
    print("=" * 60)

    try:
        from models.lightweight_smart_ai import test_integrated_ai
        test_integrated_ai()

        print("\\nâœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3 Ù…ÙƒØªÙ…Ù„Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        print("\\nğŸ¯ Ù…Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„:")
        print("â€¢ ÙÙ‡Ù… Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©")
        print("â€¢ ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…ØªÙ†ÙˆØ¹Ø©")
        print("â€¢ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù…Ø¯Ù…Ø¬Ø©")
        print("â€¢ Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©")
        print("â€¢ Ø¯Ø¹Ù… Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªØ¨Ø§Ø¹Ø¯Ø© Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ù„Ù…")
        print("â€¢ Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹ØŒ ÙŠØ¹Ù…Ù„ Ø£ÙˆÙÙ„Ø§ÙŠÙ†")

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
''')

# ğŸ¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙˆØ±ÙŠØ©

# Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3
!python test_phase3.py


# Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹

# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ ÙÙŠ Python Ù…Ø¨Ø§Ø´Ø±Ø©
!python3 -c "from models.lightweight_smart_ai import LightweightSmartAI; ai_system = LightweightSmartAI(); text = 'Ø§Ù„Ø¬Ø¨Ø± ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª.'; result = ai_system.process_text(text); print('âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©:', result)"

# ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3

# Ø¨Ø¹Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ù‰ Ù…Ø®Ø±Ø¬Ø§Øª Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù…Ø§ ÙŠÙ„ÙŠ (Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ÙˆØ§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‚Ø¯ ØªØ®ØªÙ„Ù Ù‚Ù„ÙŠÙ„Ø§Ù‹):
# ```
# ğŸ§  ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...
# âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ø®ÙÙŠÙ Ù…Ù† Ø§Ù„ØµÙØ±!
# âœ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„! (Ø£ÙˆÙÙ„Ø§ÙŠÙ† + Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…ØªØ¨Ø§Ø¹Ø¯Ø©)

# --- Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØµÙˆØµ ÙˆØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© ---
# âœ¨ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†Øµ: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ²...
# ğŸ” ÙÙ‡Ù… Ø§Ù„Ù†Øµ: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ²...
# ğŸ§  ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ù†Øµ Ù…ÙƒÙˆÙ† Ù…Ù† 17 ÙƒÙ„Ù…Ø©...
# ğŸ“„ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ²...

# ÙÙ‡Ù… Ø§Ù„Ù†Øµ 1: ['Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§']
# Ù…Ù„Ø®Øµ Ø§Ù„Ù†Øµ 1: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§.
# Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù†Øµ 1: ['Ù…Ø§ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØªØ­Ø¯Ø« Ø¹Ù†Ù‡Ø§ Ø§Ù„Ù†ØµØŸ', 'ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¬Ø¨Ø± ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŸ', 'Ø­Ù„Ù„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª.']
# ... (Ø¨Ù‚ÙŠØ© Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª)
# ```

# ğŸ‰ ØªÙ‡Ø§Ù†ÙŠÙ†Ø§! Ù„Ù‚Ø¯ Ù‚Ù…Øª Ø¨Ø¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØªØ¹Ù„ÙŠÙ…ÙŠ Ù…ØªÙƒØ§Ù…Ù„ ÙˆØ®ÙÙŠÙ Ø§Ù„ÙˆØ²Ù†!

# Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3 Ù…ÙƒØªÙ…Ù„Ø©! âœ…

# Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠÙƒ Ù†Ø¸Ø§Ù… Ù…ØªÙƒØ§Ù…Ù„ ÙŠØªÙ…ÙŠØ² Ø¨Ù€:

# Â· ğŸ§  Ø§Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù†ØµÙˆØµ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©
# Â· â“ ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© ÙˆÙ…ØªÙ†ÙˆØ¹Ø©
# Â· ğŸ“š Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù…Ø¯Ù…Ø¬Ø©
# Â· ğŸ’¬ Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ø³ÙŠØ§Ù‚ÙŠØ©
# Â· â™»ï¸ Ø¯Ø¹Ù… Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªØ¨Ø§Ø¹Ø¯Ø© Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„ØªØ¹Ù„Ù…
# Â· ğŸš€ Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹ØŒ ÙŠØ¹Ù…Ù„ Ø£ÙˆÙÙ„Ø§ÙŠÙ†

# Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©ØŸ Ù‡Ù„ ØªÙˆØ¯ ØªØ¹Ø¯ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©ØŸ Ø£Ùˆ ØªØ­Ø³ÙŠÙ† Ø£Ø¯Ø§Ø¡ Ø£ÙŠ Ø¬Ø²Ø¡ØŸ
ğŸš€ Ø¥Ù†Ø·Ù„Ù‚! Ù„Ù†Ø¨Ø¯Ø£ Ø¨Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ

ğŸ“ Ø£ÙˆÙ„Ø§Ù‹: Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª

```bash
# Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
mkdir -p models core data tests
touch models/__init__.py
touch core/__init__.py
touch data/__init__.py
```

ğŸ”§ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ Ø§Ù„Ø®ÙÙŠÙ

Ø§Ù„Ù…Ù„Ù: models/lightweight_understanding.py

```python
# models/lightweight_understanding.py
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import re
from pathlib import Path
import json

class LightweightUnderstandingModel:
    def __init__(self, model_path=None):
        self.model_size = "tiny"
        self.max_length = 512
        self.vocab_size = 10000
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        self.vocab = self.build_arabic_vocab()
        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}
        
        if model_path and Path(model_path).exists():
            self.load_model(model_path)
        else:
            self.model = self.build_lightweight_model()
            print("âœ… ØªÙ… Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ø®ÙÙŠÙ Ù…Ù† Ø§Ù„ØµÙØ±!")
    
    def build_arabic_vocab(self):
        """Ø¨Ù†Ø§Ø¡ Ù…ÙØ±Ø¯Ø§Øª Ø¹Ø±Ø¨ÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ©"""
        base_vocab = [
            '[PAD]', '[UNK]', '[CLS]', '[SEP]', 
            'Ø§Ù„', 'ÙÙŠ', 'Ù…Ù†', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ø£Ù†', 'Ù‡Ùˆ', 'Ù‡ÙŠ', 'ÙƒØ§Ù†', 'ÙŠÙƒÙˆÙ†',
            'Ù…Ø§', 'Ù‡Ø°Ø§', 'Ø°Ù„Ùƒ', 'Ù‡Ø°Ù‡', 'Ù‡Ø¤Ù„Ø§Ø¡', 'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ø°ÙŠÙ†',
            'Ù…Ø¹', 'Ø¨Ø¯ÙˆÙ†', 'Ø­ÙˆÙ„', 'Ø®Ù„Ø§Ù„', 'Ø¨ÙŠÙ†', 'Ø¶Ù…Ù†', 'Ø¹Ù†', 'ÙÙŠÙ…Ø§'
        ]
        
        # Ø¥Ø¶Ø§ÙØ© ÙƒÙ„Ù…Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
        educational_terms = [
            'ØªØ¹Ø±ÙŠÙ', 'Ù…ÙÙ‡ÙˆÙ…', 'Ø´Ø±Ø­', 'ØªØ­Ù„ÙŠÙ„', 'ØªÙ„Ø®ÙŠØµ', 'Ù†ØªÙŠØ¬Ø©', 'Ø®Ù„Ø§ØµØ©',
            'Ø±ÙŠØ§Ø¶ÙŠØ§Øª', 'Ø¹Ù„ÙˆÙ…', 'Ù„ØºØ©', 'ØªØ§Ø±ÙŠØ®', 'Ø¬ØºØ±Ø§ÙÙŠØ§', 'ÙÙŠØ²ÙŠØ§Ø¡', 'ÙƒÙŠÙ…ÙŠØ§Ø¡',
            'Ù…Ø¹Ø§Ø¯Ù„Ø©', 'Ù†Ø¸Ø±ÙŠØ©', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù…Ø¨Ø¯Ø£', 'ÙØ±Ø¶ÙŠØ©', 'ØªØ¬Ø±Ø¨Ø©', 'ØªØ­Ù‚Ù‚',
            'Ø£Ù…Ø«Ù„Ø©', 'ØªØ·Ø¨ÙŠÙ‚Ø§Øª', 'ØªÙ…Ø§Ø±ÙŠÙ†', 'Ù…Ø³Ø§Ø¦Ù„', 'Ø­Ù„', 'Ø¥Ø¬Ø§Ø¨Ø©', 'Ø³Ø¤Ø§Ù„'
        ]
        
        return base_vocab + educational_terms
    
    def build_lightweight_model(self):
        """Ø¨Ù†Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ ØªØ­ÙˆÙŠÙ„ Ø®ÙÙŠÙ Ù…Ù† Ø§Ù„ØµÙØ±"""
        class TinyTransformer(nn.Module):
            def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=4):
                super().__init__()
                self.d_model = d_model
                self.embedding = nn.Embedding(vocab_size, d_model)
                self.pos_encoding = PositionalEncoding(d_model)
                
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=d_model, 
                    nhead=nhead,
                    dim_feedforward=512,
                    dropout=0.1,
                    batch_first=True
                )
                self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
                
                # Ø±Ø£Ø³ Ù„Ù„ØªÙ„Ø®ÙŠØµ
                self.summary_head = nn.Linear(d_model, vocab_size)
                
            def forward(self, x):
                x = self.embedding(x) * np.sqrt(self.d_model)
                x = self.pos_encoding(x)
                x = self.encoder(x)
                return self.summary_head(x[:, 0, :])  # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙˆÙ„ Ø±Ù…Ø² Ù„Ù„Ø¥Ø®Ø±Ø§Ø¬
        
        return TinyTransformer(self.vocab_size)
    
    def text_to_tokens(self, text):
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ²"""
        words = re.findall(r'\b\w+\b|[^\w\s]', text.lower())
        tokens = [self.word_to_idx.get(word, self.word_to_idx['[UNK]']) for word in words]
        return tokens[:self.max_length]
    
    def understand_text(self, text):
        """ÙÙ‡Ù… Ø§Ù„Ù†Øµ ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
        print(f"ğŸ” ÙÙ‡Ù… Ø§Ù„Ù†Øµ: {text[:50]}...")
        
        if not text.strip():
            return {'main_ideas': [], 'complexity': 'low', 'word_count': 0}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø©
        sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]
        word_count = len(text.split())
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        main_ideas = self.extract_main_ideas(text)
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        complexity = self.assess_complexity(text)
        
        return {
            'main_ideas': main_ideas[:3],  # Ø£ÙˆÙ„ 3 Ø£ÙÙƒØ§Ø± Ø±Ø¦ÙŠØ³ÙŠØ© ÙÙ‚Ø·
            'complexity': complexity,
            'word_count': word_count,
            'sentence_count': len(sentences),
            'estimated_reading_time': max(1, word_count // 200)  # Ø¯Ù‚Ø§Ø¦Ù‚
        }
    
    def extract_main_ideas(self, text):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ"""
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ©
        keywords = {
            'Ù…ÙÙ‡ÙˆÙ…': 3, 'ØªØ¹Ø±ÙŠÙ': 3, 'Ù†Ø¸Ø±ÙŠØ©': 3, 'Ù‚Ø§Ù†ÙˆÙ†': 3, 'Ù…Ø¨Ø¯Ø£': 3,
            'Ø£Ù‡Ù…ÙŠØ©': 2, 'ÙØ§Ø¦Ø¯Ø©': 2, 'Ù‡Ø¯Ù': 2, 'ØºØ±Ø¶': 2,
            'Ø®ØµØ§Ø¦Øµ': 2, 'ØµÙØ§Øª': 2, 'Ù…Ù…ÙŠØ²Ø§Øª': 2,
            'Ø£Ù…Ø«Ù„Ø©': 1, 'ØªØ·Ø¨ÙŠÙ‚Ø§Øª': 1, 'Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª': 1
        }
        
        sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]
        scored_sentences = []
        
        for sentence in sentences:
            score = 0
            words = sentence.split()
            
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ø±Ø¬Ø© Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©
            if 5 <= len(words) <= 20:
                score += 2
            
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ø±Ø¬Ø© Ù„Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
            for keyword, weight in keywords.items():
                if keyword in sentence:
                    score += weight
            
            # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¯Ø±Ø¬Ø© Ù„Ù„Ø¬Ù…Ù„ Ø§Ù„Ø£ÙˆÙ„Ù‰ ÙˆØ§Ù„Ø£Ø®ÙŠØ±Ø©
            if sentence == sentences[0] or sentence == sentences[-1]:
                score += 2
            
            if score > 0:
                scored_sentences.append((sentence, score))
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¬Ù…Ù„ Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        scored_sentences.sort(key=lambda x: x[1], reverse=True)
        
        return [sentence for sentence, score in scored_sentences[:3]]
    
    def assess_complexity(self, text):
        """ØªÙ‚ÙŠÙŠÙ… Ù…Ø³ØªÙˆÙ‰ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù†Øµ"""
        words = text.split()
        word_count = len(words)
        
        if word_count < 50:
            return 'Ù…Ù†Ø®ÙØ¶'
        elif word_count < 200:
            return 'Ù…ØªÙˆØ³Ø·'
        else:
            return 'Ø¹Ø§Ù„ÙŠ'
    
    def summarize(self, text, max_sentences=3):
        """ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø§Ù„Ø¬ÙˆÙ‡Ø±"""
        print(f"ğŸ“„ ØªÙ„Ø®ÙŠØµ Ø§Ù„Ù†Øµ: {text[:50]}...")
        
        if not text.strip():
            return "Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Øµ Ù„ØªÙ„Ø®ÙŠØµÙ‡"
        
        understanding = self.understand_text(text)
        main_ideas = understanding['main_ideas']
        
        if not main_ideas:
            # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø³ØªØ·Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙÙƒØ§Ø±ØŒ Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ ÙˆØ¢Ø®Ø± Ø¬Ù…Ù„Ø©
            sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]
            if len(sentences) <= max_sentences:
                return text
            else:
                return '. '.join([sentences[0], sentences[-1]]) + '.'
        
        # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…Ù„Ø®Øµ Ù…Ù† Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        summary = '. '.join(main_ideas[:max_sentences]) + '.'
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙ†Ø³ÙŠÙ‚
        summary = re.sub(r'\s+', ' ', summary)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
        summary = summary.strip()
        
        return summary
    
    def save_model(self, path):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        torch.save({
            'model_state': self.model.state_dict(),
            'vocab': self.vocab,
            'config': {
                'vocab_size': self.vocab_size,
                'max_length': self.max_length
            }
        }, path)
        print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ: {path}")
    
    def load_model(self, path):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        checkpoint = torch.load(path, map_location='cpu')
        self.vocab = checkpoint['vocab']
        self.vocab_size = len(self.vocab)
        self.word_to_idx = {word: idx for idx, word in enumerate(self.vocab)}
        self.idx_to_word = {idx: word for idx, word in enumerate(self.vocab)}
        
        self.model = self.build_lightweight_model()
        self.model.load_state_dict(checkpoint['model_state'])
        print(f"ğŸ“‚ ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù†: {path}")

class PositionalEncoding(nn.Module):
    """ØªØ±Ù…ÙŠØ² Ù…ÙˆØ¶Ø¹ÙŠ Ø¨Ø³ÙŠØ· Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØ­ÙˆÙŠÙ„ÙŠ"""
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

# Ø§Ø®ØªØ¨Ø§Ø± ÙÙˆØ±ÙŠ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬
def test_understanding_model():
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
    model = LightweightUnderstandingModel()
    
    # Ù†ØµÙˆØµ Ø§Ø®ØªØ¨Ø§Ø±ÙŠØ©
    test_texts = [
        "Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§. ÙŠØªØ¹Ø§Ù…Ù„ Ø§Ù„Ø¬Ø¨Ø± Ù…Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„. ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø¬Ø¨Ø± ÙÙŠ Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ù…ÙˆØ² Ù…Ø«Ù„ x Ùˆ y Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…Ø¬Ù‡ÙˆÙ„Ø©.",
        "Ø§Ù„Ø®Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ÙŠØ§Ø©. Ø¬Ù…ÙŠØ¹ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ© ØªØªÙƒÙˆÙ† Ù…Ù† Ø®Ù„Ø§ÙŠØ§. Ù‡Ù†Ø§Ùƒ Ù†ÙˆØ¹Ø§Ù† Ø±Ø¦ÙŠØ³ÙŠØ§Ù† Ù…Ù† Ø§Ù„Ø®Ù„Ø§ÙŠØ§: Ø§Ù„Ø®Ù„Ø§ÙŠØ§ Ø¨Ø¯Ø§Ø¦ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø© Ù…Ø«Ù„ Ø§Ù„Ø¨ÙƒØªÙŠØ±ÙŠØ§ØŒ ÙˆØ§Ù„Ø®Ù„Ø§ÙŠØ§ Ø­Ù‚ÙŠÙ‚ÙŠØ© Ø§Ù„Ù†ÙˆØ§Ø© Ù…Ø«Ù„ Ø®Ù„Ø§ÙŠØ§ Ø§Ù„Ù†Ø¨Ø§Øª ÙˆØ§Ù„Ø­ÙŠÙˆØ§Ù†.",
        "Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ù„ØºØ© Ø³Ø§Ù…ÙŠØ© Ù‚Ø¯ÙŠÙ…Ø©ØŒ ØªØªÙ…ÙŠØ² Ø¨ØºÙ†Ù‰ Ù…ÙØ±Ø¯Ø§ØªÙ‡Ø§ ÙˆØ¯Ù‚Ø© ØªØ¹Ø¨ÙŠØ±Ù‡Ø§. ØªØ­ØªÙˆÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¹Ù„Ù‰ 28 Ø­Ø±ÙØ§Ù‹ ÙˆØªÙƒØªØ¨ Ù…Ù† Ø§Ù„ÙŠÙ…ÙŠÙ† Ø¥Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø±."
    ]
    
    for i, text in enumerate(test_texts, 1):
        print(f"\n{'='*50}")
        print(f"ğŸ“ Ø§Ù„Ù†Øµ {i}:")
        print(f"{text}")
        
        # Ø§Ù„ÙÙ‡Ù…
        understanding = model.understand_text(text)
        print(f"\nğŸ’¡ Ø§Ù„ÙÙ‡Ù…:")
        print(f"â€¢ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {understanding['main_ideas']}")
        print(f"â€¢ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {understanding['complexity']}")
        print(f"â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: {understanding['word_count']}")
        
        # Ø§Ù„ØªÙ„Ø®ÙŠØµ
        summary = model.summarize(text)
        print(f"\nğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:")
        print(f"{summary}")

if __name__ == "__main__":
    test_understanding_model()
```

ğŸ§ª Ø§Ù„Ø¢Ù† Ù„Ù†Ø®ØªØ¨Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¨Ø§Ø´Ø±Ø©!

Ø§Ù„Ù…Ù„Ù: test_phase1.py

```python
# test_phase1.py
#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ
"""

import sys
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.append(str(Path(__file__).parent))

def main():
    print("ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ")
    print("=" * 60)
    
    try:
        from models.lightweight_understanding import test_understanding_model
        test_understanding_model()
        
        print("\nâœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1 Ù…ÙƒØªÙ…Ù„Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        print("\nğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:")
        print("â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ØªÙ‚Ø±ÙŠØ¨ÙŠ: ~15MB")
        print("â€¢ Ø§Ù„Ø³Ø±Ø¹Ø©: ÙÙˆØ±ÙŠØ© (<1 Ø«Ø§Ù†ÙŠØ©)")
        print("â€¢ Ø§Ù„Ø°Ø§ÙƒØ±Ø©: ~100MB")
        print("â€¢ Ø§Ù„Ø¯Ø¹Ù…: Ø£ÙˆÙÙ„Ø§ÙŠÙ† ÙƒØ§Ù…Ù„")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
        print("\nğŸ”§ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª torch:")
        print("pip install torch")

if __name__ == "__main__":
    main()
```

ğŸ“¦ Ù…Ù„Ù Ø§Ù„Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„Ø®ÙÙŠÙ

Ø§Ù„Ù…Ù„Ù: requirements_lightweight.txt

```txt
torch>=1.9.0
numpy>=1.21.0
scikit-learn>=1.0.0
```

ğŸ¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙÙˆØ±ÙŠØ©

Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±

```bash
# ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª
ls -la models/

# Ø´ØºÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±
python test_phase1.py
```

Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø°Ø§ ÙˆØ§Ø¬Ù‡Øª Ù…Ø´Ø§ÙƒÙ„ Ù…Ø¹ torch

```bash
# ØªØ«Ø¨ÙŠØª torch Ø®ÙÙŠÙ (Ø¨Ø¯ÙˆÙ† CUDA)
pip install torch --index-url https://download.pytorch.org/whl/cpu

# Ø£Ùˆ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ³ØªØ®Ø¯Ù… GPU:
pip install torch torchvision torchaudio
```

Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠ

```python
# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ ÙÙŠ Python Ù…Ø¨Ø§Ø´Ø±Ø©
python3 -c "
from models.lightweight_understanding import LightweightUnderstandingModel
model = LightweightUnderstandingModel()
result = model.understand_text('Ø§Ù„Ø¬Ø¨Ø± ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ù…ØªØºÙŠØ±Ø§Øª')
print('âœ… Ø§Ù„Ù†ØªÙŠØ¬Ø©:', result)
"
```

ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1

Ø¨Ø¹Ø¯ ØªØ´ØºÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ù‰:

```
ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù… ÙˆØ§Ù„ØªÙ„Ø®ÙŠØµ
============================================================
ğŸ” ÙÙ‡Ù… Ø§Ù„Ù†Øµ: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ²...
ğŸ’¡ Ø§Ù„ÙÙ‡Ù…:
â€¢ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: ['Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§']
â€¢ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: Ù…ØªÙˆØ³Ø·
â€¢ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª: 25

ğŸ“„ Ø§Ù„Ù…Ù„Ø®Øµ:
Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ© ÙˆØ§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ†Ù‡Ø§.
```

ğŸ‰ ØªÙ‡Ø§Ù†ÙŠÙ†Ø§! Ù„Ù‚Ø¯ Ø£Ù†Ø´Ø£Øª Ø£ÙˆÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ø¹Ø±Ø¨ÙŠ Ø®ÙÙŠÙ!

Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1 Ù…ÙƒØªÙ…Ù„Ø© âœ…

Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠÙƒ:

Â· ğŸ§  Ù†Ù…ÙˆØ°Ø¬ ÙÙ‡Ù… Ù†ØµÙŠ Ø¹Ø±Ø¨ÙŠ
Â· ğŸ“„ Ù†Ø¸Ø§Ù… ØªÙ„Ø®ÙŠØµ Ø°ÙƒÙŠ
Â· âš¡ Ø®ÙÙŠÙ ÙˆØ³Ø±ÙŠØ¹
Â· ğŸ’¾ ÙŠØ¹Ù…Ù„ Ø£ÙˆÙÙ„Ø§ÙŠÙ†

Ù‡Ù„ ÙŠØ¹Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ØŸ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬Ø§Ù‡Ø²Ø§Ù‹ØŒ ÙÙ„Ù†Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©! ğŸš€ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ© - Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙƒØ§Ù…Ù„

ğŸ“ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©

Ø§Ù„Ù…Ù„Ù: models/smart_question_system.py

```python
# models/smart_question_system.py
import re
import random
from typing import List, Dict
import numpy as np

class SmartQuestionGenerator:
    def __init__(self):
        # Ù‚ÙˆØ§Ù„Ø¨ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© Ø¨Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ©
        self.question_templates = {
            'comprehension': [
                "Ù…Ø§ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØªØ­Ø¯Ø« Ø¹Ù†Ù‡Ø§ Ø§Ù„Ù†ØµØŸ",
                "Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ¹Ù†ÙŠÙ‡ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©: '{phrase}' ÙÙŠ Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ",
                "Ø¹Ø±Ù/Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… '{concept}' Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ.",
                "Ù…Ø§ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† Ø¯Ø±Ø§Ø³Ø© '{concept}'ØŸ",
                "Ø§Ø°ÙƒØ± Ø«Ù„Ø§Ø« Ù†Ù‚Ø§Ø· Ø±Ø¦ÙŠØ³ÙŠØ© ÙˆØ±Ø¯Øª ÙÙŠ Ø§Ù„Ù†Øµ."
            ],
            'application': [
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ '{concept}' ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŸ",
                "Ù…Ø§ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù… '{concept}' Ù…Ù† ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø­ÙŠØ§Ø©ØŸ",
                "ÙƒÙŠÙ ØªØ­Ù„ Ù…Ø´ÙƒÙ„Ø© Ù…Ø´Ø§Ø¨Ù‡Ø© Ù„Ù…Ø´ÙƒÙ„Ø© '{concept}' Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø§ ØªØ¹Ù„Ù…ØªÙ‡ØŸ",
                "ØµÙ…Ù… ØªØ¬Ø±Ø¨Ø© Ø¨Ø³ÙŠØ·Ø© ØªÙˆØ¶Ø­ '{concept}'.",
                "Ù…Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ù„ÙÙƒØ±Ø© '{concept}'ØŸ"
            ],
            'analysis': [
                "Ù…Ø§ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù Ø¨ÙŠÙ† '{concept1}' Ùˆ '{concept2}'ØŸ",
                "Ù„Ù…Ø§Ø°Ø§ ÙŠØ¹ØªØ¨Ø± '{concept}' Ù…Ù‡Ù…Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØŸ",
                "Ù…Ø§ Ø§Ù„Ø¢Ø«Ø§Ø±/Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªØ±ØªØ¨Ø© Ø¹Ù„Ù‰ ØªØ·Ø¨ÙŠÙ‚ '{action}'ØŸ",
                "Ø­Ù„Ù„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø© Ø¨ÙŠÙ† '{concept1}' Ùˆ '{concept2}'.",
                "Ù…Ø§ Ø§Ù„Ø£Ø³Ø¨Ø§Ø¨ Ø§Ù„ØªÙŠ Ø£Ø¯Øª Ø¥Ù„Ù‰ '{result}' Ø­Ø³Ø¨ Ø§Ù„Ù†ØµØŸ"
            ],
            'evaluation': [
                "Ù…Ø§ Ø±Ø£ÙŠÙƒ ÙÙŠ ÙØ¹Ø§Ù„ÙŠØ© '{concept}' Ù„Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§ØªØŸ",
                "Ù‚ÙŠÙ… Ø£Ù‡Ù…ÙŠØ© '{concept}' ÙÙŠ Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ø¹Ù„Ù…ÙŠ.",
                "Ù…Ø§ Ù…Ø¯Ù‰ ØµØ­Ø© Ø§Ù„Ø¹Ø¨Ø§Ø±Ø©: '{phrase}'ØŸ Ø¨Ø±Ø± Ø¥Ø¬Ø§Ø¨ØªÙƒ.",
                "Ù…Ø§ Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ù„Ù€ '{concept}'ØŸ",
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† ØªØ·Ø¨ÙŠÙ‚ '{concept}'ØŸ"
            ],
            'synthesis': [
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬ '{concept1}' Ù…Ø¹ '{concept2}' Ù„Ø¥Ù†ØªØ§Ø¬ ÙÙƒØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©ØŸ",
                "Ù…Ø§ Ø§Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ù„Ø§ØµÙ‡Ø§ Ù…Ù† Ø§Ù„Ù†ØµØŸ",
                "ØµÙ…Ù… Ø®Ø·Ø© Ø¨Ø­Ø«ÙŠØ© Ù„Ø¯Ø±Ø§Ø³Ø© '{concept}'.",
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ± '{concept}' Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹ØŸ",
                "Ù…Ø§ Ø§Ù„Ø­Ù„ÙˆÙ„ Ø§Ù„Ù…Ø¨ØªÙƒØ±Ø© Ù„Ù…Ø´ÙƒÙ„Ø© '{problem}'ØŸ"
            ]
        }
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ù…Ø¹ Ø£ÙˆØ²Ø§Ù†Ù‡Ø§
        self.edu_keywords = {
            'ØªØ¹Ø±ÙŠÙ': 3, 'Ù…ÙÙ‡ÙˆÙ…': 3, 'Ù†Ø¸Ø±ÙŠØ©': 3, 'Ù‚Ø§Ù†ÙˆÙ†': 3, 'Ù…Ø¨Ø¯Ø£': 3,
            'Ø®ØµØ§Ø¦Øµ': 2, 'ØµÙØ§Øª': 2, 'Ù…Ù…ÙŠØ²Ø§Øª': 2, 'Ø£Ù†ÙˆØ§Ø¹': 2, 'Ø£Ù‚Ø³Ø§Ù…': 2,
            'Ø£Ù‡Ù…ÙŠØ©': 2, 'ÙØ§Ø¦Ø¯Ø©': 2, 'Ù‡Ø¯Ù': 2, 'ØºØ±Ø¶': 2,
            'Ø£Ù…Ø«Ù„Ø©': 1, 'ØªØ·Ø¨ÙŠÙ‚Ø§Øª': 1, 'Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª': 1, 'Ù†Ù…Ø§Ø°Ø¬': 1,
            'Ù†ØªÙŠØ¬Ø©': 2, 'Ø®Ù„Ø§ØµØ©': 2, 'Ø§Ø³ØªÙ†ØªØ§Ø¬': 2, 'ØªØ­Ù„ÙŠÙ„': 2
        }
        
        # Ù…ØµØ·Ù„Ø­Ø§Øª ØªØ¹Ù„ÙŠÙ…ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
        self.common_concepts = [
            'Ø§Ù„Ø¬Ø¨Ø±', 'Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©', 'Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª', 'Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª', 'Ø§Ù„Ø¯ÙˆØ§Ù„',
            'Ø§Ù„Ø®Ù„ÙŠØ©', 'Ø§Ù„ØªÙƒØ§Ø«Ø±', 'Ø§Ù„ÙˆØ±Ø§Ø«Ø©', 'Ø§Ù„ØªØ·ÙˆØ±', 'Ø§Ù„Ø¨ÙŠØ¦Ø©',
            'Ø§Ù„Ù‚ÙˆØ©', 'Ø§Ù„Ø·Ø§Ù‚Ø©', 'Ø§Ù„Ø­Ø±ÙƒØ©', 'Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¡', 'Ø§Ù„Ù…ØºÙ†Ø§Ø·ÙŠØ³ÙŠØ©',
            'Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª', 'Ø§Ù„Ù…Ø±ÙƒØ¨Ø§Øª', 'Ø§Ù„Ø¹Ù†Ø§ØµØ±', 'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¯ÙˆØ±ÙŠ'
        ]
        
    def generate_questions(self, text: str, num_questions: int = 3, question_types: List[str] = None) -> List[Dict]:
        """ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠØ© Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‡Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ"""
        
        print(f"ğŸ§  ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ù†Øµ Ù…ÙƒÙˆÙ† Ù…Ù† {len(text.split())} ÙƒÙ„Ù…Ø©...")
        
        if not text.strip():
            return [{
                'question': "Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ù‚Ø¯Ù… ÙØ§Ø±Øº. Ù‡Ù„ ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ‚Ø¯ÙŠÙ… Ù†Øµ Ù„Ù„Ù…Ù†Ø§Ù‚Ø´Ø©ØŸ",
                'type': 'general', 
                'difficulty': 'low',
                'concepts_used': []
            }]

        # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
        concepts = self.extract_concepts(text)
        phrases = self.extract_key_phrases(text)
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ù…ÙØ§Ù‡ÙŠÙ…ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù…ØµØ·Ù„Ø­Ø§Øª Ø¹Ø§Ù…Ø©
        if not concepts:
            concepts = self.common_concepts[:3]
        
        if not concepts and not phrases:
            return [{
                'question': "Ø§Ù„Ù†Øµ Ø¨Ø³ÙŠØ·. Ù‡Ù„ ØªØ­ØªØ§Ø¬ Ù„Ø´Ø±Ø­ Ø£Ùˆ ØªÙ„Ø®ÙŠØµ Ø¥Ø¶Ø§ÙÙŠØŸ", 
                'type': 'general', 
                'difficulty': 'low',
                'concepts_used': []
            }]

        # ØªØ­Ø¯ÙŠØ¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        if question_types is None:
            question_types = ['comprehension', 'application', 'analysis']
        
        questions = []
        attempts = 0
        max_attempts = num_questions * 3
        
        while len(questions) < num_questions and attempts < max_attempts:
            attempts += 1
            
            # Ø§Ø®ØªÙŠØ§Ø± Ù†ÙˆØ¹ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ Ù…Ù† Ø§Ù„Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
            q_type = random.choice(question_types)
            template = random.choice(self.question_templates[q_type])
            
            question_text = template
            used_concepts = []
            
            try:
                # Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙˆØ§Ù„Ø¨ Ø¨Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…/Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©
                if '{concept}' in template and concepts:
                    concept = random.choice(concepts)
                    question_text = template.format(concept=concept)
                    used_concepts.append(concept)
                
                elif '{phrase}' in template and phrases:
                    phrase = random.choice(phrases)
                    question_text = template.format(phrase=phrase)
                    used_concepts.append(phrase)
                
                elif '{concept1}' in template and '{concept2}' in template and len(concepts) >= 2:
                    concept1, concept2 = random.sample(concepts, 2)
                    question_text = template.format(concept1=concept1, concept2=concept2)
                    used_concepts.extend([concept1, concept2])
                
                elif '{action}' in template and concepts:
                    action = random.choice(concepts)
                    question_text = template.format(action=action)
                    used_concepts.append(action)
                
                elif '{result}' in template and concepts:
                    result = random.choice(concepts)
                    question_text = template.format(result=result)
                    used_concepts.append(result)
                
                elif '{problem}' in template and concepts:
                    problem = random.choice(concepts)
                    question_text = template.format(problem=problem)
                    used_concepts.append(problem)
                
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ù…Ù„Ø¡ Ø§Ù„Ù‚Ø§Ù„Ø¨ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙÙ‡ÙˆÙ… Ø¹Ø´ÙˆØ§Ø¦ÙŠ
                if ('{' in question_text and '}' in question_text) and concepts:
                    concept = random.choice(concepts)
                    question_text = question_text.format(
                        concept=concept,
                        concept1=concept,
                        concept2=concept,
                        phrase=phrases[0] if phrases else "Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù‚Ø·Ø©",
                        action=concept,
                        result=concept,
                        problem=concept
                    )
                    used_concepts.append(concept)
                
                # ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø©
                if not any(q['question'] == question_text for q in questions):
                    questions.append({
                        'question': question_text,
                        'type': q_type,
                        'difficulty': self.assess_difficulty(q_type, len(used_concepts)),
                        'concepts_used': used_concepts
                    })
                    
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ Ø³Ø¤Ø§Ù„: {e}")
                continue
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ØŒ Ù†Ø¶ÙŠÙ Ø£Ø³Ø¦Ù„Ø© Ø¹Ø§Ù…Ø©
        if len(questions) < num_questions:
            general_questions = [
                "Ù…Ø§ Ø§Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù†ØµØŸ",
                "ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙÙŠ Ø§Ù„Ø¯Ø±Ø§Ø³Ø©ØŸ",
                "Ù…Ø§ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© Ø§Ù„ØªÙŠ ØªÙˆØ¯ Ù…Ø¹Ø±ÙØªÙ‡Ø§ Ø¹Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ØŸ"
            ]
            
            for gq in general_questions[:num_questions - len(questions)]:
                questions.append({
                    'question': gq,
                    'type': 'general',
                    'difficulty': 'medium',
                    'concepts_used': []
                })
        
        return questions[:num_questions]
    
    def extract_concepts(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©"""
        concepts = []
        sentences = re.split(r'[.!ØŸ]', text)
        
        for sentence in sentences:
            words = sentence.split()
            for i, word in enumerate(words):
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
                if word in self.edu_keywords:
                    # Ù†Ø£Ø®Ø° Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ØªØ§Ù„ÙŠØ© ÙƒÙ…ÙØ§Ù‡ÙŠÙ… Ù…Ø­ØªÙ…Ù„Ø©
                    potential_concepts = []
                    
                    # Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø©
                    if i + 1 < len(words):
                        next_word = words[i + 1].strip(' ,:;-')
                        if len(next_word) > 2 and not next_word.isdigit():
                            potential_concepts.append(next_word)
                    
                    # Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© (Ù„Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ù…Ø«Ù„ "Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø¬Ø¨Ø±")
                    if i > 0:
                        prev_word = words[i - 1].strip(' ,:;-')
                        if len(prev_word) > 2 and not prev_word.isdigit():
                            potential_concepts.append(prev_word)
                    
                    concepts.extend(potential_concepts)
        
        # ØªÙ†Ø¸ÙŠÙ ÙˆØªØµÙÙŠØ© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        cleaned_concepts = []
        for concept in concepts:
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
            common_words = {'Ù‡Ùˆ', 'ÙÙŠ', 'Ù…Ù†', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ø£Ù†', 'ÙƒØ§Ù†'}
            if (concept not in common_words and 
                len(concept) > 2 and 
                not concept.isdigit()):
                cleaned_concepts.append(concept)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªØ±ØªÙŠØ¨
        seen = set()
        unique_concepts = []
        for concept in cleaned_concepts:
            if concept not in seen:
                seen.add(concept)
                unique_concepts.append(concept)
        
        return unique_concepts[:8]  # 8 Ù…ÙØ§Ù‡ÙŠÙ… ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰
    
    def extract_key_phrases(self, text: str) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ù…Ù† Ø§Ù„Ù†Øµ"""
        sentences = [s.strip() for s in re.split(r'[.!ØŸ]', text) if s.strip()]
        
        if not sentences:
            return []
        
        key_phrases = []
        
        # Ø£ÙˆÙ„ Ø¬Ù…Ù„Ø© (ØºØ§Ù„Ø¨Ø§Ù‹ ØªÙƒÙˆÙ† ØªÙ…Ù‡ÙŠØ¯ÙŠØ©)
        if sentences:
            key_phrases.append(sentences[0])
        
        # Ø¢Ø®Ø± Ø¬Ù…Ù„Ø© (ØºØ§Ù„Ø¨Ø§Ù‹ ØªÙƒÙˆÙ† Ø®Ù„Ø§ØµØ©)
        if len(sentences) > 1:
            key_phrases.append(sentences[-1])
        
        # Ø£Ø·ÙˆÙ„ Ø¬Ù…Ù„Ø© (Ù‚Ø¯ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ØªÙØ§ØµÙŠÙ„ Ù…Ù‡Ù…Ø©)
        if len(sentences) > 2:
            longest_sentence = max(sentences[1:-1] if len(sentences) > 2 else sentences, key=len)
            if longest_sentence not in key_phrases:
                key_phrases.append(longest_sentence)
        
        # Ø¬Ù…Ù„ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ©
        for sentence in sentences:
            if any(keyword in sentence for keyword in self.edu_keywords):
                if sentence not in key_phrases and len(key_phrases) < 5:
                    key_phrases.append(sentence)
        
        return key_phrases[:4]
    
    def assess_difficulty(self, question_type: str, num_concepts: int) -> str:
        """ØªÙ‚ÙŠÙŠÙ… ØµØ¹ÙˆØ¨Ø© Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù†ÙˆØ¹Ù‡ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…"""
        base_difficulty = {
            'comprehension': 'low',
            'application': 'medium', 
            'analysis': 'high',
            'evaluation': 'high',
            'synthesis': 'high',
            'general': 'medium'
        }
        
        difficulty = base_difficulty.get(question_type, 'medium')
        
        # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØµØ¹ÙˆØ¨Ø© Ù…Ø¹ Ø²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        if num_concepts >= 2 and difficulty == 'medium':
            return 'high'
        elif num_concepts >= 3:
            return 'high'
        
        return difficulty
    
    def generate_questions_by_type(self, text: str, question_type: str, num_questions: int = 2) -> List[Dict]:
        """ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…Ù† Ù†ÙˆØ¹ Ù…Ø­Ø¯Ø¯"""
        return self.generate_questions(text, num_questions, [question_type])
    
    def get_question_statistics(self, questions: List[Dict]) -> Dict:
        """Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø©"""
        type_count = {}
        difficulty_count = {}
        all_concepts = []
        
        for q in questions:
            # Ø¹Ø¯ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
            type_count[q['type']] = type_count.get(q['type'], 0) + 1
            
            # Ø¹Ø¯ Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„ØµØ¹ÙˆØ¨Ø©
            difficulty_count[q['difficulty']] = difficulty_count.get(q['difficulty'], 0) + 1
            
            # Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
            all_concepts.extend(q['concepts_used'])
        
        # Ø¥Ø²Ø§Ù„Ø© ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
        unique_concepts = list(set(all_concepts))
        
        return {
            'total_questions': len(questions),
            'type_distribution': type_count,
            'difficulty_distribution': difficulty_count,
            'unique_concepts_used': unique_concepts,
            'total_concepts_used': len(all_concepts)
        }

# Ø§Ø®ØªØ¨Ø§Ø± ÙÙˆØ±ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…
def test_question_generator():
    print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©...")
    print("=" * 60)
    
    generator = SmartQuestionGenerator()
    
    test_text = (
        "Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§ØªØŒ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„. "
        "Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ù…ØªØºÙŠØ± Ù‡Ùˆ Ø±Ù…Ø² (Ø¹Ø§Ø¯Ø©Ù‹ x Ø£Ùˆ y) ÙŠÙ…Ø«Ù„ Ù‚ÙŠÙ…Ø© Ù…Ø¬Ù‡ÙˆÙ„Ø©. "
        "Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ø¬Ø¨Ø± ØªÙƒÙ…Ù† ÙÙŠ ØªØ·Ø¨ÙŠÙ‚Ø§ØªÙ‡ Ø§Ù„ÙˆØ§Ø³Ø¹Ø© ÙÙŠ Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø© ÙˆØ§Ù„ØªÙ…ÙˆÙŠÙ„ØŒ ÙˆÙ‡Ùˆ Ù…Ø¨Ø¯Ø£ Ø£Ø³Ø§Ø³ÙŠ Ù„ÙÙ‡Ù… Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¡. "
        "ØªØªØ¶Ù…Ù† Ø®ØµØ§Ø¦Øµ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª Ø§Ù„Ø¬Ø¨Ø±ÙŠØ© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¬Ù…Ø¹ ÙˆØ§Ù„Ø·Ø±Ø­ ÙˆØ§Ù„Ø¶Ø±Ø¨ Ø¹Ù„Ù‰ Ø·Ø±ÙÙŠ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø© Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§Ø²Ù†. "
        "Ù†Ø¸Ø±ÙŠØ© ÙÙŠØ«Ø§ØºÙˆØ±Ø³ Ù‡ÙŠ Ø£Ø­Ø¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ù‡Ø§Ù…Ø© Ù„Ù„Ø¬Ø¨Ø± ÙÙŠ Ø§Ù„Ù‡Ù†Ø¯Ø³Ø©."
    )
    
    print("\nğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ¯Ø±:")
    print(test_text)
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…
    concepts = generator.extract_concepts(test_text)
    print(f"\nğŸ’¡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: {concepts}")
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    phrases = generator.extract_key_phrases(test_text)
    print(f"ğŸ“Œ Ø§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {phrases}")
    
    # ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©
    print(f"\nâ“ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø© (5 Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©):")
    questions = generator.generate_questions(test_text, 5)
    
    for i, q in enumerate(questions, 1):
        print(f"  {i}. [{q['type'].upper()}] - Ù…Ø³ØªÙˆÙ‰: {q['difficulty']}")
        print(f"     Ø§Ù„Ø³Ø¤Ø§Ù„: {q['question']}")
        if q['concepts_used']:
            print(f"     Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: {', '.join(q['concepts_used'])}")
        print()
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    stats = generator.get_question_statistics(questions)
    print(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:")
    print(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©: {stats['total_questions']}")
    print(f"   â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹: {stats['type_distribution']}")
    print(f"   â€¢ ØªÙˆØ²ÙŠØ¹ Ø§Ù„ØµØ¹ÙˆØ¨Ø©: {stats['difficulty_distribution']}")
    print(f"   â€¢ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©: {len(stats['unique_concepts_used'])} Ù…ÙÙ‡ÙˆÙ…")
    
    # Ø§Ø®ØªØ¨Ø§Ø± ØªÙˆÙ„ÙŠØ¯ Ø£Ø³Ø¦Ù„Ø© Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
    print(f"\nğŸ¯ Ø£Ø³Ø¦Ù„Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙ‚Ø· (2 Ø³Ø¤Ø§Ù„):")
    analysis_questions = generator.generate_questions_by_type(test_text, 'analysis', 2)
    for i, q in enumerate(analysis_questions, 1):
        print(f"  {i}. {q['question']}")

if __name__ == "__main__":
    test_question_generator()
```

ğŸ§ª Ù…Ù„Ù Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2

Ø§Ù„Ù…Ù„Ù: test_phase2.py

```python
# test_phase2.py
#!/usr/bin/env python3
"""
Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©
"""

import sys
from pathlib import Path

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.append(str(Path(__file__).parent))

def main():
    print("ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©")
    print("=" * 60)
    
    try:
        # Ø§Ø®ØªØ¨Ø§Ø± Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
        from models.smart_question_system import test_question_generator
        test_question_generator()
        
        print("\nâœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ù…ÙƒØªÙ…Ù„Ø© Ø¨Ù†Ø¬Ø§Ø­!")
        print("\nğŸ¯ Ù…Ù…ÙŠØ²Ø§Øª Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø©:")
        print("â€¢ 5 Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø© (ÙÙ‡Ù…ØŒ ØªØ·Ø¨ÙŠÙ‚ØŒ ØªØ­Ù„ÙŠÙ„ØŒ ØªÙ‚ÙŠÙŠÙ…ØŒ ØªØ±ÙƒÙŠØ¨)")
        print("â€¢ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª")
        print("â€¢ ØªÙ‚ÙŠÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©")
        print("â€¢ Ù…Ù†Ø¹ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø³Ø¦Ù„Ø©")
        print("â€¢ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©")
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1
        print("\nğŸ”— Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1:")
        from models.lightweight_understanding import LightweightUnderstandingModel
        from models.smart_question_system import SmartQuestionGenerator
        
        # Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù…
        understanding_model = LightweightUnderstandingModel()
        # Ù…ÙˆÙ„Ø¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        question_generator = SmartQuestionGenerator()
        
        test_text = "Ø¹Ù„Ù… Ø§Ù„Ø£Ø­ÙŠØ§Ø¡ ÙŠØ¯Ø±Ø³ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ© ÙˆØ·Ø±Ù‚ ØªÙƒØ§Ø«Ø±Ù‡Ø§ ÙˆØªØ·ÙˆØ±Ù‡Ø§. Ø§Ù„Ø®Ù„ÙŠØ© Ù‡ÙŠ Ø§Ù„ÙˆØ­Ø¯Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© Ù„Ù„Ø­ÙŠØ§Ø©."
        
        print(f"ğŸ“ Ø§Ù„Ù†Øµ: {test_text}")
        
        # Ø§Ù„ÙÙ‡Ù… Ø£ÙˆÙ„Ø§Ù‹
        understanding = understanding_model.understand_text(test_text)
        print(f"ğŸ’¡ Ø§Ù„Ø£ÙÙƒØ§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: {understanding['main_ideas']}")
        
        # Ø«Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©
        questions = question_generator.generate_questions(test_text, 3)
        print(f"â“ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø©:")
        for i, q in enumerate(questions, 1):
            print(f"  {i}. {q['question']}")
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
```

ğŸ¯ Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙÙˆØ±ÙŠØ©

Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ´ØºÙŠÙ„ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2

```bash
python test_phase2.py
```

Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„ÙŠØ¯ÙˆÙŠ Ø§Ù„Ø³Ø±ÙŠØ¹

```python
# Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ ÙÙŠ Python Ù…Ø¨Ø§Ø´Ø±Ø©
python3 -c "
from models.smart_question_system import SmartQuestionGenerator
generator = SmartQuestionGenerator()
text = 'Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª Ø¹Ù„Ù… ÙŠØ¯Ø±Ø³ Ø§Ù„ÙƒÙ… ÙˆØ§Ù„Ø¹Ø¯Ø¯ ÙˆØ§Ù„Ù‡Ù†Ø¯Ø³Ø©. Ø§Ù„Ø¬Ø¨Ø± ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³ Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª.'
questions = generator.generate_questions(text, 3)
for i, q in enumerate(questions, 1):
    print(f'{i}. {q[\\\"question\\\"]}')
"
```

ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2

Ø¨Ø¹Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„ØŒ ÙŠØ¬Ø¨ Ø£Ù† ØªØ±Ù‰:

```
ğŸš€ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø°ÙƒÙŠØ©
============================================================
ğŸ“ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ØµØ¯Ø±: Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† ÙØ±ÙˆØ¹ Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§ØªØŒ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª...

ğŸ’¡ Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ… Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©: ['Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª', 'Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª', 'Ø§Ù„Ø¯ÙˆØ§Ù„', 'Ø§Ù„Ø¬Ø¨Ø±']

â“ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ØªÙˆÙ„Ø¯Ø© (5 Ø£Ø³Ø¦Ù„Ø© Ù…ØªÙ†ÙˆØ¹Ø©):
  1. [COMPREHENSION] - Ù…Ø³ØªÙˆÙ‰: medium
     Ø§Ù„Ø³Ø¤Ø§Ù„: Ù…Ø§ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØªØ­Ø¯Ø« Ø¹Ù†Ù‡Ø§ Ø§Ù„Ù†ØµØŸ
     Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: []

  2. [APPLICATION] - Ù…Ø³ØªÙˆÙ‰: medium  
     Ø§Ù„Ø³Ø¤Ø§Ù„: ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¬Ø¨Ø± ÙÙŠ Ø§Ù„Ø­ÙŠØ§Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©ØŸ
     Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: ['Ø§Ù„Ø¬Ø¨Ø±']

  3. [ANALYSIS] - Ù…Ø³ØªÙˆÙ‰: high
     Ø§Ù„Ø³Ø¤Ø§Ù„: Ù…Ø§ Ø£ÙˆØ¬Ù‡ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ø§Ø®ØªÙ„Ø§Ù Ø¨ÙŠÙ† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§ØªØŸ
     Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…: ['Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª', 'Ø§Ù„Ù…Ø¹Ø§Ø¯Ù„Ø§Øª']
```

ğŸ‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ù…ÙƒØªÙ…Ù„Ø©! âœ…

Ø§Ù„Ø¢Ù† Ù„Ø¯ÙŠÙƒ Ù†Ø¸Ø§Ù… Ø£Ø³Ø¦Ù„Ø© Ø°ÙƒÙŠ Ù…ØªÙƒØ§Ù…Ù„ ÙŠØªÙ…ÙŠØ² Ø¨Ù€:

Â· ğŸ¯ 5 Ù…Ø³ØªÙˆÙŠØ§Øª ØªÙÙƒÙŠØ± (Ø¨Ù„ÙˆÙ… Ø§Ù„ØªØµÙ†ÙŠÙÙŠ)
Â· ğŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø°ÙƒÙŠ Ù„Ù„Ù…ÙØ§Ù‡ÙŠÙ… ÙˆØ§Ù„Ø¹Ø¨Ø§Ø±Ø§Øª
Â· ğŸ“Š ØªÙ‚ÙŠÙŠÙ… ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØµØ¹ÙˆØ¨Ø©
Â· ğŸš€ ØªÙƒØ§Ù…Ù„ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙÙ‡Ù…
Â· ğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø© Ø¹Ù† Ø§Ù„Ø£Ø³Ø¦Ù„Ø©

Ù‡Ù„ ÙŠØ¹Ù…Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨Ø´ÙƒÙ„ Ø¬ÙŠØ¯ØŸ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¬Ø§Ù‡Ø²Ø§Ù‹ØŒ ÙÙ„Ù†Ù†ØªÙ‚Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„ÙƒØ§Ù…Ù„! ğŸš€ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø®ÙÙŠÙ - Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

ğŸ“ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø³Ù†: models/lightweight_smart_ai.py

```python
# models/lightweight_smart_ai.py
import os
import json
import re
import time
from pathlib import Path
from typing import Dict, List, Any
from datetime import datetime

# Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ù†Ø¬Ø²Ø© Ù…Ù† Ø§Ù„Ù…Ø±Ø§Ø­Ù„ Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©
from models.lightweight_understanding import LightweightUnderstandingModel
from models.smart_question_system import SmartQuestionGenerator

class LightweightSmartAI:
    def __init__(self, knowledge_base_path=None, enable_spaced_repetition=True):
        print("ğŸ§  ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„...")
        
        # 1. ØªØ­Ù…ÙŠÙ„ Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø®ÙÙŠÙ
        self.understanding_model = LightweightUnderstandingModel()
        self.question_generator = SmartQuestionGenerator()
        
        # 2. Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙˆØ§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
        self.conversation_memory = []
        self.user_progress = {}
        self.spaced_repetition_enabled = enable_spaced_repetition
        self.knowledge_base = self.load_enhanced_knowledge_base(knowledge_base_path)
        
        # 3. Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…ØªØ¨Ø§Ø¹Ø¯Ø©
        self.review_schedule = {}
        self.concept_mastery = {}
        
        print("âœ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¹Ù…Ù„! (Ø£ÙˆÙÙ„Ø§ÙŠÙ† + Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…ØªØ¨Ø§Ø¹Ø¯Ø©)")
    
    def load_enhanced_knowledge_base(self, path):
        """ØªØ­Ù…ÙŠÙ„ Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù…ÙˆØ³Ø¹Ø© ÙˆÙ…Ø­Ø³Ù†Ø©"""
        if path and Path(path).exists():
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        # Ù‚Ø§Ø¹Ø¯Ø© Ù…Ø¹Ø±ÙØ© Ù…ÙˆØ³Ø¹Ø© Ù…Ø¹ ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©
        return {
            "Ø§Ù„Ø¬Ø¨Ø±": {
                "definition": "Ø§Ù„Ø¬Ø¨Ø± Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø±ÙŠØ§Ø¶ÙŠØ§Øª ÙŠØ¯Ø±Ø³
