import os
import json
import argparse
import subprocess
import time
import shutil
from pathlib import Path
from typing import Dict, Any, Tuple
from tenacity import retry, stop_after_attempt, wait_fixed
from contextlib import contextmanager
import psutil
import gc
import torch

print("ğŸš€ Starting Optimized AI Code Processor...")

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    import bitsandbytes
    AI_AVAILABLE = True
    print("âœ… AI libraries imported successfully (8-bit enabled)")
except ImportError as e:
    print(f"âŒ AI libraries not available. Ensure torch, transformers, and bitsandbytes are installed: {e}")
    AI_AVAILABLE = False

try:
    import psutil
    import humanize
    MEMORY_MONITOR_AVAILABLE = True
except ImportError:
    print("âš ï¸ psutil/humanize not available - memory monitoring disabled")
    MEMORY_MONITOR_AVAILABLE = False


class OptimizedAIProcessor:
    def __init__(self, model_size: str = "1.3b", batch_limit: int = 6, resume: bool = True, 
                 optimize_memory: bool = True, safe_mode: bool = True):
        self.model_size = model_size
        self.batch_limit = batch_limit
        self.resume = resume
        self.optimize_memory = optimize_memory
        self.safe_mode = safe_mode
        
        self.progress_file = "progress.json"
        self.processed_files = set()
        self.progress_data: Dict[str, Any] = {}
        self.original_content_cache: Dict[str, str] = {}
        
        self.all_files = [] 
        
        # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ÙØ­Øµ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù‚Ø±Øµ Ø¹Ù†Ø¯ Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        if self.safe_mode:
            self.check_disk_space()
        
        self.setup_model()
        self.load_progress()
    
    def check_disk_space(self):
        """ÙØ­Øµ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù‚Ø±Øµ ÙˆØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±."""
        try:
            total, used, free = shutil.disk_usage("/")
            free_gb = free // (2**30)
            print(f"ğŸ’¾ Disk Space - Total: {total//(2**30)}GB, Used: {used//(2**30)}GB, Free: {free_gb}GB")
            
            if free_gb < 2:  # Ø£Ù‚Ù„ Ù…Ù† 2GB Ù…ØªØ¨Ù‚ÙŠ
                print("ğŸš¨ WARNING: Low disk space! Cleaning cache...")
                self.clean_disk_cache()
        except Exception as e:
            print(f"âš ï¸ Could not check disk space: {e}")
    
    def clean_disk_cache(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ù„Ù„Ù‚Ø±Øµ."""
        try:
            print("ğŸ§¹ Starting comprehensive disk cleanup...")
            
            # ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© pip Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            result = subprocess.run(["pip", "cache", "purge"], capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                print("âœ… pip cache cleaned")
            
            # ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© huggingface Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            result = subprocess.run(["huggingface-cli", "delete-cache", "-f"], capture_output=True, text=True, timeout=60)
            if result.returncode == 0:
                print("âœ… huggingface cache cleaned")
            
            # ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© torch Ø§Ù„Ù…Ø¤Ù‚ØªØ©
            torch_cache = os.path.expanduser("~/.cache/torch")
            if os.path.exists(torch_cache):
                subprocess.run(["rm", "-rf", torch_cache], capture_output=True)
                print("âœ… torch cache cleaned")
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¤Ù‚ØªØ© Ø§Ù„Ø£Ø®Ø±Ù‰
            temp_dirs = ['/tmp', '/var/tmp']
            for temp_dir in temp_dirs:
                if os.path.exists(temp_dir):
                    subprocess.run(["find", temp_dir, "-type", "f", "-name", "*.tmp", "-delete"], capture_output=True)
                    subprocess.run(["find", temp_dir, "-type", "f", "-name", "*.temp", "-delete"], capture_output=True)
            
            print("âœ… Comprehensive disk cleanup completed")
            
            # ÙØ­Øµ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ
            total, used, free = shutil.disk_usage("/")
            free_gb = free // (2**30)
            print(f"ğŸ’¾ After cleanup - Free: {free_gb}GB")
                
        except Exception as e:
            print(f"âš ï¸ Cache cleaning failed: {e}")
    
    def setup_model(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù†Ø© ÙˆØªØ­Ù…ÙŠÙ„ 8-Ø¨Øª."""
        if not AI_AVAILABLE:
            print("âŒ AI not available - running in fallback mode (copy only)")
            self.model = None
            return
            
        try:
            model_name = "deepseek-ai/deepseek-coder-1.3b-base"
            print(f"ğŸ”§ Loading optimized model: {model_name}")
            
            # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù‚Ø±Øµ Ù‚Ø¨Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬
            if self.safe_mode:
                self.check_disk_space()
            
            # â­ Ø§Ù„Ø¥ØµÙ„Ø§Ø­: ØªØ¹Ø·ÙŠÙ„ hf_transfer Ù…Ø¤Ù‚ØªÙ‹Ø§ Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
            original_env = os.environ.get('HF_HUB_ENABLE_HF_TRANSFER')
            os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = '0'
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                trust_remote_code=True,
                padding_side="left"
            )
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                load_in_8bit=True,
                device_map="auto",
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            
            # â­ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©
            if original_env is not None:
                os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = original_env
            else:
                os.environ.pop('HF_HUB_ENABLE_HF_TRANSFER', None)
            
            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            
            print("âœ… Optimized AI model loaded successfully!")
            
        except Exception as e:
            print(f"âŒ Model loading failed: {e}")
            print("ğŸ”„ Falling back to copy mode without AI processing")
            self.model = None

    def clear_memory(self):
        """ØªÙ†Ø¸ÙŠÙ Ø°Ø§ÙƒØ±Ø© GPU/CUDA Ø¨Ø¹Ø¯ ÙƒÙ„ Ø¯ÙØ¹Ø©."""
        if self.optimize_memory and AI_AVAILABLE and torch.cuda.is_available():
            mem_allocated = torch.cuda.memory_allocated()
            mem_cached = torch.cuda.memory_reserved()
            torch.cuda.empty_cache()
            
            if MEMORY_MONITOR_AVAILABLE:
                print(f"ğŸ§¹ Clearing CUDA cache: Allocated={humanize.naturalsize(mem_allocated)}, Cached={humanize.naturalsize(mem_cached)}")
        
        if self.optimize_memory:
            import gc
            gc.collect()
        
        if MEMORY_MONITOR_AVAILABLE:
            process = psutil.Process(os.getpid())
            print(f"   Current CPU RAM usage: {humanize.naturalsize(process.memory_info().rss)}")

    @contextmanager
    def timer(self, operation_name: str):
        """Ù…Ø±Ø§Ù‚Ø¨ Ø¨Ø³ÙŠØ· Ù„ÙˆÙ‚Øª Ø§Ù„ØªØ´ØºÙŠÙ„."""
        start = time.time()
        yield
        print(f"â±ï¸ {operation_name} completed in {time.time() - start:.2f}s")
    
    def discover_files(self):
        """Ø§ÙƒØªØ´Ø§Ù Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…Ø±Ø§Ø¯ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ù…Ø¹ Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯."""
        extensions = {'.py', '.js', '.html', '.css', '.md', '.txt', '.json'}
        exclude_dirs = {'.git', 'node_modules', '__pycache__', '.venv', 'cleaned_repo', '.github'}
        
        files = []
        for root, dirs, filenames in os.walk('.'):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            for file in filenames:
                file_path = Path(root) / file
                if any(exclude in file_path.parts for exclude in exclude_dirs):
                     continue
                
                if (file_path.suffix.lower() in extensions and 
                    file_path.stat().st_size <= 50 * 1024):
                    files.append(str(file_path))
        return files

    def initialize_progress(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø³Ø¬Ù„ Ø§Ù„ØªÙ‚Ø¯Ù…."""
        self.all_files = self.discover_files()
        self.progress_data = {
            'total': len(self.all_files),
            'processed': 0,
            'progress': 0.0,
            'batches_completed': 0,
            'processed_files': []
        }
        print(f"ğŸ“ Found {len(self.all_files)} files to process")

    def load_progress(self):
        """ØªØ­Ù…ÙŠÙ„ Ø³Ø¬Ù„ Ø§Ù„ØªÙ‚Ø¯Ù…."""
        self.all_files = self.discover_files()
        
        if self.resume and Path(self.progress_file).exists():
            try:
                with open(self.progress_file, 'r') as f:
                    self.progress_data = json.load(f)
                self.processed_files = set(self.progress_data.get('processed_files', []))
                processed_count = self.progress_data.get('processed', 0)
                total_count = self.progress_data.get('total', 0)
                print(f"ğŸ”„ Resumed: {processed_count}/{total_count} files processed")
            except Exception as e:
                print(f"âš ï¸ Failed to load progress: {e}")
                self.initialize_progress()
        else:
            self.initialize_progress()
    
    def get_next_batch(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØ¹Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©."""
        remaining = [f for f in self.all_files if f not in self.processed_files]
        
        python_files = [f for f in remaining if f.endswith('.py')]
        js_files = [f for f in remaining if f.endswith('.js')]
        other_files = [f for f in remaining if not f.endswith(('.py', '.js'))]
        
        prioritized_files = python_files + js_files + other_files
        
        # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ‚Ù„ÙŠÙ„ Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø© ÙÙŠ safe mode
        if self.safe_mode and len(prioritized_files) > 4:
            print("ğŸ›¡ï¸ Safe mode: Reducing batch size for stability")
            return prioritized_files[:4]
        
        return prioritized_files[:self.batch_limit]

    def build_prompt(self, content: str, file_type: str) -> str:
        """Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§Ù„Ù…ÙØ­Ø³Ù‘ÙÙ† Ù„Ù†Ù…ÙˆØ°Ø¬ DeepSeek."""
        
        instruction = (
            "You are an expert code optimizer and cleaner. "
            "Your task is to analyze the following code, "
            "refactor it to be more efficient, and standard compliant. "
            "Crucially, the output code must be fully compliant with Black, isort, and Flake8 standards. "
            "The line length must not exceed 100 characters. "
            "Return ONLY the cleaned code block and nothing else."
        )
        
        if file_type == '.py':
             return f"{instruction}\n\n```{file_type.strip('.')}\n{content.strip()}\n```"
        
        base_prompt = f"Review and optimize the following {file_type.strip('.') if file_type != '.py' else 'code'}. Output ONLY the clean, final code block and nothing else:\n\n"
        return f"{base_prompt}```{file_type.strip('.')}\n{content.strip()}\n```"

    def process_file_without_ai(self, content: str, file_path: str) -> str:
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† AI - ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ ÙÙ‚Ø·."""
        file_suffix = Path(file_path).suffix
        
        if file_suffix == '.py':
            try:
                # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ù…Ø¤Ù‚ØªÙ‹Ø§ Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ†Ø³ÙŠÙ‚
                temp_path = Path('temp_processing.py')
                temp_path.write_text(content, encoding='utf-8')
                
                # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªØ·Ø¨ÙŠÙ‚ autopep8 Ø£ÙˆÙ„Ø§Ù‹ Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù†Ø­ÙˆÙŠØ©
                subprocess.run(["autopep8", "--in-place", "--aggressive", str(temp_path)], check=False, timeout=30)
                
                # Ø«Ù… ØªØ·Ø¨ÙŠÙ‚ black
                result = subprocess.run(["black", str(temp_path), "--quiet"], capture_output=True, text=True, timeout=30)
                if result.returncode != 0:
                    print(f"âš ï¸ Black failed for {file_path}, using autopep8 only")
                    # Ø¥Ø°Ø§ ÙØ´Ù„ BlackØŒ Ù†Ø³ØªØ®Ø¯Ù… autopep8 ÙÙ‚Ø·
                    subprocess.run(["autopep8", "--in-place", str(temp_path)], check=False, timeout=30)
                
                formatted_content = temp_path.read_text(encoding='utf-8')
                temp_path.unlink()  # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
                
                print(f"ğŸ“ Formatted {file_path} (no AI)")
                return formatted_content
            except Exception as e:
                print(f"âš ï¸ Formatting failed for {file_path}: {e}")
                return content
        else:
            return content

    def quality_check(self, code: str, file_type: str) -> str:
        """ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø£Ø³Ø§Ø³ÙŠ Ù„Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙÙ†ØªÙØ¬."""
        if file_type == '.py':
            # â­ Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            if "fÙ…Ø¹Ø§Ù„Ø¬Ø©" in code:
                code = code.replace("fÙ…Ø¹Ø§Ù„Ø¬Ø©", "f\"Ù…Ø¹Ø§Ù„Ø¬Ø©\"")
            if "f'Ù…Ø¹Ø§Ù„Ø¬Ø©" in code:
                code = code.replace("f'Ù…Ø¹Ø§Ù„Ø¬Ø©", "f\"Ù…Ø¹Ø§Ù„Ø¬Ø©\"")
            
            if "random.choice" in code:
                code = code.replace("['','', '', '','','',,'']", "['choice1', 'choice2', 'choice3']")
                code = code.replace("['','']", "['item1', 'item2']")
            
            # Ø¥Ø²Ø§Ù„Ø© Ø£ÙŠ Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…ØªØ¨Ù‚ÙŠ Ù…Ù† AI
            lines = code.split('\n')
            cleaned_lines = []
            for line in lines:
                if any(keyword in line for keyword in ['Review and optimize', 'cleaned code block', 'Return ONLY']):
                    continue
                cleaned_lines.append(line)
            code = '\n'.join(cleaned_lines)
            
        return code

    def extract_clean_code(self, result: str, file_type: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ø¸ÙŠÙ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¨Ø³Ø·Ø© ÙˆØ¢Ù…Ù†Ø©."""
        
        # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ†Ø¸ÙŠÙ Ø£ÙƒØ«Ø± Ø´Ù…ÙˆÙ„ÙŠØ© Ù„Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª
        if '```' not in result:
            # Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ ÙƒØªÙ„Ø© ÙƒÙˆØ¯ØŒ Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙˆØ¯ Ù…Ø¨Ø§Ø´Ø±Ø©
            lines = result.split('\n')
            code_lines = []
            for line in lines:
                if any(keyword in line.lower() for keyword in ['explanation', 'note:', 'here is', 'cleaned code', 'the cleaned', 'review and optimize']):
                    continue
                if line.strip() and not line.startswith('```'):
                    code_lines.append(line)
            return '\n'.join(code_lines).strip()
            
        try:
            parts = result.split('```')
            cleaned = parts[-2]  # Ù†Ø£Ø®Ø° Ø§Ù„ÙƒØªÙ„Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø£Ø®ÙŠØ±Ø© (Ø¹Ø§Ø¯Ø©Ù‹ Ø§Ù„ÙƒÙˆØ¯)
            
            lang_prefix = file_type.strip('.')
            if cleaned.lower().startswith(lang_prefix):
                cleaned = cleaned[len(lang_prefix):]
            
            lines = [
                line for line in cleaned.split('\n')
                if not any(keyword in line.lower() for keyword in 
                          ['explanation', 'note:', 'here is', 'cleaned code', 'the cleaned', 'review and optimize'])
            ]
            
            return '\n'.join(lines).strip()
        
        except Exception:
            return result.strip()

    def _generate_ai_code(self, full_prompt: str, file_type: str) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯ Ø¨ÙˆØ§Ø³Ø·Ø© DeepSeek."""
        if self.model is None:
            return "Error: AI not available"
            
        with self.timer("AI Generation"):
            try:
                inputs = self.tokenizer(
                    full_prompt, 
                    return_tensors="pt", 
                    truncation=True, 
                    max_length=2048,
                    padding=True
                )
                
                with torch.no_grad():
                    with torch.cuda.amp.autocast(dtype=torch.bfloat16):
                        outputs = self.model.generate(
                            inputs.input_ids,
                            attention_mask=inputs.attention_mask,
                            max_new_tokens=600,  
                            temperature=0.2,
                            do_sample=True,
                            pad_token_id=self.tokenizer.pad_token_id,
                            eos_token_id=self.tokenizer.eos_token_id,
                            repetition_penalty=1.2,
                            no_repeat_ngram_size=3
                        )
                
                result = self.tokenizer.decode(
                    outputs[0], 
                    skip_special_tokens=True,
                    clean_up_tokenization_spaces=True
                )
                
                cleaned_code = self.extract_clean_code(result, file_type)
                cleaned_code = self.quality_check(cleaned_code, file_type)
                
                return cleaned_code.strip()
                
            except Exception as e:
                print(f"âš ï¸ AI processing failed during generation: {e}")
                return "Error: AI failed to generate valid code"

    def run_pylint(self, file_path: Path) -> Tuple[bool, str]:
        """ØªØ´ØºÙŠÙ„ Pylint ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ù†Ø­ÙˆÙŠ."""
        try:
            result = subprocess.run(
                ["pylint", str(file_path), "--disable=all", "--enable=E0001,F0001", "--output-format=text"],
                capture_output=True, text=True, timeout=30
            )
            
            if "E0001" in result.stdout or "F0001" in result.stdout:
                error_message = "\n".join([line for line in result.stdout.split('\n') if "error" in line.lower() or "fatal" in line.lower()])
                return True, error_message.strip()
            
            return False, ""
        except Exception as e:
            return True, f"Pylint failed to execute: {e}"

    def validate_python_syntax(self, code: str) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¬Ù…Ù„Ø© ÙÙŠ Python."""
        try:
            compile(code, '<string>', 'exec')
            return True
        except SyntaxError as e:
            print(f"âš ï¸ Syntax error detected: {e}")
            return False

    @retry(stop=stop_after_attempt(2), wait=wait_fixed(5))
    def repair_with_ai_loop(self, original_content: str, file_path: str, is_retry: bool = False) -> str:
        """Ø­Ù„Ù‚Ø© Ø§Ù„ØªØµØ­ÙŠØ­ Ø§Ù„Ø°Ø§ØªÙŠ Ø§Ù„Ù…ÙˆØ¬Ù‡ Ø¨Ø§Ù„Ø£Ø®Ø·Ø§Ø¡."""
        # â­ Ø¥Ø°Ø§ ÙƒØ§Ù† AI ØºÙŠØ± Ù…ØªØ§Ø­ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if self.model is None:
            return self.process_file_without_ai(original_content, file_path)
            
        file_suffix = Path(file_path).suffix
        temp_output_path = Path('cleaned_repo') / file_path
        
        if is_retry:
            print(f"ğŸ”„ Attempting AI self-correction for {file_path} (Retry 1/1)...")
            is_error, error_msg = self.run_pylint(temp_output_path)
            
            if not is_error and file_suffix == '.py':
                try:
                    subprocess.run(["autopep8", "--in-place", "--aggressive", str(temp_output_path)], check=True, timeout=30)
                    subprocess.run(["black", str(temp_output_path), "--check"], check=True, timeout=60)
                    print("âœ… autopep8 fix successful. Passing.")
                    return temp_output_path.read_text(encoding='utf-8')
                except Exception:
                    pass
            
            prompt = (
                "The previous code failed a Black/Pylint check with the following error:\n"
                f"--- ERROR: {error_msg} ---\n"
                "You MUST fix this precise error and return ONLY the syntactically correct, Black-compliant code block."
            )
            content_to_process = temp_output_path.read_text(encoding='utf-8', errors='ignore')
            full_prompt = f"{prompt}\n\n```{file_suffix.strip('.')}\n{content_to_process.strip()}\n```"
            
        else:
            content_to_process = original_content
            full_prompt = self.build_prompt(content_to_process[:2500], file_suffix)

        cleaned_content = self._generate_ai_code(full_prompt, file_suffix)
        
        temp_output_path.parent.mkdir(parents=True, exist_ok=True)
        temp_output_path.write_text(cleaned_content, encoding='utf-8')

        try:
            # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¬Ù…Ù„Ø© Ø£ÙˆÙ„Ø§Ù‹
            if file_suffix == '.py' and not self.validate_python_syntax(cleaned_content):
                raise Exception("Generated code has syntax errors")
                
            subprocess.run(["black", str(temp_output_path), "--check"], check=True, timeout=60)
            print(f"âœ… AI output for {file_path} passed Black/Pylint check.")
            return cleaned_content

        except subprocess.CalledProcessError:
            if not is_retry:
                raise Exception("Black failed: initiating AI self-correction loop.")
            else:
                print(f"âŒ AI self-correction for {file_path} failed after retry. Reverting to original content.")
                return original_content 
        
        except Exception as e:
            print(f"âŒ Unexpected failure in repair loop: {e}. Reverting to original content.")
            return original_content

    def process_batch(self):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª."""
        # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ÙØ­Øµ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù‚Ø±Øµ Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡
        if self.safe_mode:
            self.check_disk_space()
        
        batch_files = self.get_next_batch()
        
        if not batch_files:
            print("ğŸ‰ All files processed!")
            return True
        
        print(f"ğŸ”„ Processing {len(batch_files)} files...")
        if self.model is None:
            print("âš ï¸ Running in FALLBACK MODE - AI not available, using basic formatting only")
        
        success_count = 0
        with self.timer(f"Processing Batch of {len(batch_files)} files"):
            for file_path in batch_files:
                try:
                    content = Path(file_path).read_text(encoding='utf-8', errors='ignore')
                    self.original_content_cache[file_path] = content
                    
                    cleaned_content = self.repair_with_ai_loop(content, file_path)
                    
                    output_path = Path('cleaned_repo') / file_path
                    output_path.write_text(cleaned_content, encoding='utf-8')
                    
                    self.processed_files.add(file_path)
                    success_count += 1
                    print(f"âœ… Finalized: {file_path}")
                    
                    # â­ Ø§Ù„ØªØ­Ø³ÙŠÙ†: ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ ÙƒÙ„ Ù…Ù„Ù ÙÙŠ safe mode
                    if self.optimize_memory:
                        self.clear_memory()
                    
                except Exception as e:
                    print(f"âŒ Failed to process: {file_path} - {e}")
            
            if AI_AVAILABLE and self.optimize_memory:
                self.clear_memory()

        self.progress_data['processed'] = len(self.processed_files)
        self.progress_data['progress'] = (self.progress_data['processed'] / self.progress_data['total']) * 100
        self.progress_data['batches_completed'] += 1
        self.progress_data['processed_files'] = list(self.processed_files)
        
        self.save_progress()
        
        print(f"ğŸ“Š Batch completed: {success_count}/{len(batch_files)} files successful")
        print(f"ğŸ“ˆ Overall progress: {self.progress_data['processed']}/{self.progress_data['total']} ({self.progress_data['progress']:.1f}%)")
        
        return False
    
    def save_progress(self):
        """Ø­ÙØ¸ Ø³Ø¬Ù„ Ø§Ù„ØªÙ‚Ø¯Ù…."""
        with open(self.progress_file, 'w') as f:
            json.dump(self.progress_data, f, indent=2)


def main():
    parser = argparse.ArgumentParser(description='AI Code Processor with optimized settings')
    parser.add_argument('--model', default='1.3b', help='Model size')
    parser.add_argument('--limit', type=int, default=6, help='Batch size limit')
    parser.add_argument('--resume', type=bool, default=True, help='Resume from previous progress')
    # â­ Ø§Ù„Ø¥ØµÙ„Ø§Ø­: Ø¥Ø¶Ø§ÙØ© Ø§Ù„ÙˆØ³ÙŠØ·Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
    parser.add_argument('--optimize-memory', action='store_true', help='Enable memory optimization')
    parser.add_argument('--safe-mode', action='store_true', help='Enable safe mode with disk monitoring')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Starting Optimized AI Processor (Batch: {args.limit}, Model: {args.model})")
    if args.optimize_memory:
        print("ğŸ§  Memory optimization: ENABLED")
    if args.safe_mode:
        print("ğŸ›¡ï¸ Safe mode: ENABLED")
    
    processor = OptimizedAIProcessor(
        model_size=args.model,
        batch_limit=args.limit,
        resume=args.resume,
        optimize_memory=args.optimize_memory,
        safe_mode=args.safe_mode
    )
    
    is_complete = processor.process_batch()
    
    if is_complete:
        print("ğŸ‰ Processing complete! All files have been processed.")
    else:
        print(f"ğŸ”„ Batch completed. Ready for next batch.")


if __name__ == "__main__":
    main()
